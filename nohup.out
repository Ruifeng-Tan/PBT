The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2025-04-18 15:21:24,733] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 15:21:24,733] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 15:21:25,685] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-04-18 15:21:25,685] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-04-18 15:21:25,746] [INFO] [comm.py:652:init_distributed] cdb=None
{'task_name': 'battery_life_prediction', 'is_training': 1, 'model_id': 'TunePara', 'model_comment': '50to1', 'model': 'BatteryMoE_bottleneck_all', 'LLM_path': '/data/LLMs/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95', 'center_path': './Centenr_vectors', 'seed': 2021, 'dataset': 'exp', 'data': 'Dataset_BatteryLifeLLM_original', 'root_path': '/data/trf/python_works/BatteryLife/dataset', 'data_path': 'ETTh1.csv', 'features': 'MS', 'target': 'OT', 'loader': 'modal', 'freq': 'h', 'checkpoints': '/data/LLMs/checkpoints', 'num_process': 2, 'early_cycle_threshold': 100, 'seq_len': 1, 'label_len': 50, 'seasonal_patterns': 'Monthly', 'd_llm': 4096, 'enc_in': 3, 'dec_in': 1, 'c_out': 1, 'd_model': 128, 'n_heads': 8, 'noDKP_layers': 1, 'bottleneck_factor': 8, 'e_layers': 1, 'd_layers': 1, 'd_ff': 128, 'moving_avg': 25, 'factor': 3, 'dropout': 0.05, 'embed': 'Cycle', 'activation': 'relu', 'output_attention': False, 'patch_len': 10, 'stride': 10, 'output_num': 1, 'class_num': 8, 'weighted_loss': False, 'num_workers': 16, 'itr': 1, 'T0': 2, 'train_epochs': 100, 'least_epochs': 15, 'batch_size': 58, 'patience': 5, 'learning_rate': 5e-05, 'wd': 0.0, 'des': 'Exp', 'loss': 'MSE', 'lradj': 'type1', 'lradj_factor': 0.5, 'pct_start': 0.2, 'use_amp': False, 'llm_layers': 32, 'top_p': 0.5, 'accumulation_steps': 1, 'mlp': 0, 'warm_up_epoches': 0, 'num_views': 4, 'num_general_experts': 4, 'num_experts': 20, 'cathode_experts': 11, 'temperature_experts': 15, 'format_experts': 11, 'anode_experts': 11, 'noisy_gating': False, 'topK': -1, 'importance_weight': 1.0, 'use_ReMoE': False, 'use_guide': False, 'gamma': 1.0, 'use_LB': False, 'Pretrained_model_path': '', 'wo_DKPrompt': False, 'charge_discharge_length': 300, 'alpha1': 0.15, 'alpha2': 0.1}
  0%|          | 0/5 [00:00<?, ?it/s]  0%|          | 0/5 [00:00<?, ?it/s] 20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  1.11s/it] 20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  1.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.04it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:02,  1.11it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.02it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:03<00:02,  1.01s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.20it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.30it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.19it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.27it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.16it/s]
Loading training samples......
Loading vali samples......
  0%|          | 0/4 [00:00<?, ?it/s]  0%|          | 0/4 [00:00<?, ?it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.80it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.61it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  2.93it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  2.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  1.92it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  1.91it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.23it/s]
  0%|          | 0/3 [00:00<?, ?it/s]Loading test samples......
  0%|          | 0/3 [00:00<?, ?it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:01,  1.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.41it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.38it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.62it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.56it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.54it/s]
success delete /data/LLMs/checkpoints/BatteryMoE_bottleneck_all_sl1_lr5e-05_dm128_nh8_el1_dl1_df128_llmLayers32_lradjtype1_datasetexp_guideFalse_LBFalse_lossMSE_wd0.0_wlFalse_noDKPL1_dr0.05_bf8_NumE20_K-1_seed2021-50to1
[2025-04-18 15:21:34,851] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
wandb: Currently logged in as: trf824449964 (hkustgz-batteryinformatics). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.9
wandb: Run data is saved locally in /data/trf/python_works/BatteryMoE/wandb/run-20250418_152135-4objvd75
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 2025-04-18 15:21:24
wandb: â­ï¸ View project at https://wandb.ai/hkustgz-batteryinformatics/BatteryMoE_FixPrompt
wandb: ðŸš€ View run at https://wandb.ai/hkustgz-batteryinformatics/BatteryMoE_FixPrompt/runs/4objvd75
{'Total': 15834817, 'Trainable': 15834817, 'Percent': 1.0}
Layer: gate.0.weight | Number of parameters: 786432
Layer: gate.0.bias | Number of parameters: 192
Layer: flattenIntraCycleLayer.view_experts.0.experts.0.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.0.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.0.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.0.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.1.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.1.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.1.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.1.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.2.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.2.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.2.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.2.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.3.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.3.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.3.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.3.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.4.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.4.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.4.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.4.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.5.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.5.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.5.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.5.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.6.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.6.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.6.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.6.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.7.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.7.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.7.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.7.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.8.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.8.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.8.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.8.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.9.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.9.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.9.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.9.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.10.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.10.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.10.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.10.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.11.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.11.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.11.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.11.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.12.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.12.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.12.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.12.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.13.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.13.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.13.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.13.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.14.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.14.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.14.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.14.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.15.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.15.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.15.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.15.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.16.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.16.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.16.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.16.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.17.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.17.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.17.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.17.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.18.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.18.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.18.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.18.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.19.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.19.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.19.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.19.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.20.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.20.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.20.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.20.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.21.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.21.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.21.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.21.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.22.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.22.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.22.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.22.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.23.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.23.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.23.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.23.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.24.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.24.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.24.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.24.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.25.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.25.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.25.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.25.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.26.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.26.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.26.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.26.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.27.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.27.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.27.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.27.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.28.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.28.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.28.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.28.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.29.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.29.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.29.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.29.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.30.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.30.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.30.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.30.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.31.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.31.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.31.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.31.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.32.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.32.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.32.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.32.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.33.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.33.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.33.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.33.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.34.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.34.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.34.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.34.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.35.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.35.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.35.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.35.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.36.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.36.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.36.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.36.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.37.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.37.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.37.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.37.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.38.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.38.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.38.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.38.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.39.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.39.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.39.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.39.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.40.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.40.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.40.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.40.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.41.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.41.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.41.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.41.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.42.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.42.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.42.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.42.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.43.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.43.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.43.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.43.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.44.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.44.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.44.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.44.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.45.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.45.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.45.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.45.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.46.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.46.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.46.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.46.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.view_experts.0.experts.47.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.view_experts.0.experts.47.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.view_experts.0.experts.47.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.view_experts.0.experts.47.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.general_experts.0.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.general_experts.0.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.general_experts.0.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.general_experts.0.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.general_experts.1.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.general_experts.1.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.general_experts.1.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.general_experts.1.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.general_experts.2.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.general_experts.2.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.general_experts.2.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.general_experts.2.2.bias | Number of parameters: 128
Layer: flattenIntraCycleLayer.general_experts.3.1.weight | Number of parameters: 14400
Layer: flattenIntraCycleLayer.general_experts.3.1.bias | Number of parameters: 16
Layer: flattenIntraCycleLayer.general_experts.3.2.weight | Number of parameters: 2048
Layer: flattenIntraCycleLayer.general_experts.3.2.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.0.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.0.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.0.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.1.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.1.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.1.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.2.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.2.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.2.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.3.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.3.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.3.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.4.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.4.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.4.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.5.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.5.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.5.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.6.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.6.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.6.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.7.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.7.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.7.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.8.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.8.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.8.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.9.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.9.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.9.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.10.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.10.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.10.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.11.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.11.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.11.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.12.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.12.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.12.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.13.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.13.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.13.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.14.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.14.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.14.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.15.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.15.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.15.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.16.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.16.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.16.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.17.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.17.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.17.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.18.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.18.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.18.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.19.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.19.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.19.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.20.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.20.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.20.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.21.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.21.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.21.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.22.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.22.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.22.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.23.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.23.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.23.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.24.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.24.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.24.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.25.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.25.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.25.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.26.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.26.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.26.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.27.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.27.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.27.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.28.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.28.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.28.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.29.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.29.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.29.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.30.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.30.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.30.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.31.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.31.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.31.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.32.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.32.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.32.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.33.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.33.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.33.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.34.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.34.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.34.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.35.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.35.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.35.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.36.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.36.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.36.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.37.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.37.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.37.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.38.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.38.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.38.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.39.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.39.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.39.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.40.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.40.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.40.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.41.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.41.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.41.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.42.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.42.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.42.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.43.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.43.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.43.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.44.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.44.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.44.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.45.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.45.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.45.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.46.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.46.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.46.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.view_experts.0.experts.47.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.47.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.view_experts.0.experts.47.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.general_experts.0.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.general_experts.0.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.general_experts.0.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.general_experts.1.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.general_experts.1.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.general_experts.1.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.general_experts.2.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.general_experts.2.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.general_experts.2.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.general_experts.3.in_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.general_experts.3.out_linear.weight | Number of parameters: 16384
Layer: intra_MoE_layers.0.general_experts.3.out_linear.bias | Number of parameters: 128
Layer: intra_MoE_layers.0.norm.weight | Number of parameters: 128
Layer: intra_MoE_layers.0.norm.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.0.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.0.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.0.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.0.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.1.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.1.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.1.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.1.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.2.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.2.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.2.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.2.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.3.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.3.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.3.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.3.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.4.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.4.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.4.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.4.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.5.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.5.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.5.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.5.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.6.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.6.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.6.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.6.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.7.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.7.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.7.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.7.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.8.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.8.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.8.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.8.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.9.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.9.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.9.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.9.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.10.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.10.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.10.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.10.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.11.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.11.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.11.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.11.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.12.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.12.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.12.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.12.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.13.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.13.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.13.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.13.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.14.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.14.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.14.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.14.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.15.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.15.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.15.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.15.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.16.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.16.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.16.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.16.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.17.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.17.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.17.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.17.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.18.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.18.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.18.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.18.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.19.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.19.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.19.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.19.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.20.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.20.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.20.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.20.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.21.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.21.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.21.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.21.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.22.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.22.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.22.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.22.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.23.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.23.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.23.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.23.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.24.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.24.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.24.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.24.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.25.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.25.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.25.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.25.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.26.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.26.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.26.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.26.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.27.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.27.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.27.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.27.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.28.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.28.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.28.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.28.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.29.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.29.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.29.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.29.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.30.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.30.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.30.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.30.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.31.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.31.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.31.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.31.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.32.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.32.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.32.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.32.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.33.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.33.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.33.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.33.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.34.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.34.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.34.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.34.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.35.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.35.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.35.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.35.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.36.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.36.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.36.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.36.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.37.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.37.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.37.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.37.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.38.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.38.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.38.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.38.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.39.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.39.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.39.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.39.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.40.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.40.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.40.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.40.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.41.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.41.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.41.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.41.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.42.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.42.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.42.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.42.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.43.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.43.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.43.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.43.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.44.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.44.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.44.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.44.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.45.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.45.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.45.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.45.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.46.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.46.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.46.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.46.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.view_experts.0.experts.47.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.view_experts.0.experts.47.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.view_experts.0.experts.47.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.view_experts.0.experts.47.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.general_experts.0.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.general_experts.0.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.general_experts.0.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.general_experts.0.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.general_experts.1.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.general_experts.1.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.general_experts.1.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.general_experts.1.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.general_experts.2.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.general_experts.2.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.general_experts.2.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.general_experts.2.2.bias | Number of parameters: 128
Layer: flattenInterCycleLayer.general_experts.3.0.weight | Number of parameters: 2048
Layer: flattenInterCycleLayer.general_experts.3.0.bias | Number of parameters: 16
Layer: flattenInterCycleLayer.general_experts.3.2.weight | Number of parameters: 204800
Layer: flattenInterCycleLayer.general_experts.3.2.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.0.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.0.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.0.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.1.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.1.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.1.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.2.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.2.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.2.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.3.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.3.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.3.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.4.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.4.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.4.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.5.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.5.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.5.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.6.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.6.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.6.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.7.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.7.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.7.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.8.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.8.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.8.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.9.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.9.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.9.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.10.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.10.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.10.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.11.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.11.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.11.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.12.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.12.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.12.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.13.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.13.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.13.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.14.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.14.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.14.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.15.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.15.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.15.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.16.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.16.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.16.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.17.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.17.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.17.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.18.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.18.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.18.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.19.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.19.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.19.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.20.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.20.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.20.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.21.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.21.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.21.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.22.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.22.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.22.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.23.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.23.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.23.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.24.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.24.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.24.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.25.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.25.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.25.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.26.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.26.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.26.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.27.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.27.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.27.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.28.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.28.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.28.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.29.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.29.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.29.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.30.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.30.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.30.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.31.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.31.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.31.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.32.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.32.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.32.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.33.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.33.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.33.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.34.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.34.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.34.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.35.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.35.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.35.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.36.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.36.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.36.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.37.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.37.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.37.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.38.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.38.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.38.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.39.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.39.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.39.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.40.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.40.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.40.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.41.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.41.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.41.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.42.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.42.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.42.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.43.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.43.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.43.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.44.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.44.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.44.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.45.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.45.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.45.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.46.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.46.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.46.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.view_experts.0.experts.47.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.47.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.view_experts.0.experts.47.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.general_experts.0.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.general_experts.0.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.general_experts.0.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.general_experts.1.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.general_experts.1.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.general_experts.1.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.general_experts.2.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.general_experts.2.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.general_experts.2.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.general_experts.3.in_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.general_experts.3.out_linear.weight | Number of parameters: 16384
Layer: inter_MoE_layers.0.general_experts.3.out_linear.bias | Number of parameters: 128
Layer: inter_MoE_layers.0.norm.weight | Number of parameters: 128
Layer: inter_MoE_layers.0.norm.bias | Number of parameters: 128
Layer: regression_head.projection.0.weight | Number of parameters: 128
Layer: regression_head.projection.0.bias | Number of parameters: 1
Trainable parameters are: ['gate.0.weight', 'gate.0.bias', 'flattenIntraCycleLayer.view_experts.0.experts.0.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.0.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.0.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.0.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.1.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.1.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.1.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.1.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.2.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.2.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.2.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.2.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.3.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.3.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.3.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.3.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.4.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.4.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.4.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.4.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.5.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.5.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.5.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.5.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.6.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.6.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.6.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.6.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.7.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.7.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.7.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.7.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.8.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.8.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.8.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.8.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.9.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.9.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.9.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.9.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.10.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.10.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.10.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.10.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.11.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.11.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.11.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.11.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.12.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.12.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.12.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.12.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.13.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.13.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.13.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.13.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.14.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.14.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.14.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.14.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.15.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.15.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.15.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.15.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.16.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.16.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.16.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.16.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.17.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.17.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.17.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.17.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.18.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.18.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.18.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.18.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.19.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.19.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.19.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.19.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.20.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.20.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.20.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.20.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.21.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.21.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.21.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.21.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.22.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.22.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.22.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.22.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.23.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.23.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.23.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.23.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.24.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.24.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.24.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.24.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.25.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.25.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.25.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.25.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.26.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.26.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.26.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.26.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.27.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.27.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.27.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.27.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.28.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.28.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.28.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.28.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.29.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.29.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.29.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.29.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.30.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.30.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.30.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.30.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.31.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.31.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.31.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.31.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.32.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.32.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.32.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.32.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.33.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.33.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.33.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.33.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.34.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.34.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.34.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.34.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.35.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.35.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.35.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.35.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.36.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.36.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.36.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.36.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.37.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.37.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.37.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.37.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.38.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.38.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.38.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.38.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.39.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.39.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.39.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.39.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.40.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.40.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.40.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.40.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.41.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.41.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.41.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.41.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.42.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.42.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.42.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.42.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.43.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.43.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.43.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.43.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.44.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.44.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.44.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.44.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.45.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.45.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.45.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.45.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.46.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.46.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.46.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.46.2.bias', 'flattenIntraCycleLayer.view_experts.0.experts.47.1.weight', 'flattenIntraCycleLayer.view_experts.0.experts.47.1.bias', 'flattenIntraCycleLayer.view_experts.0.experts.47.2.weight', 'flattenIntraCycleLayer.view_experts.0.experts.47.2.bias', 'flattenIntraCycleLayer.general_experts.0.1.weight', 'flattenIntraCycleLayer.general_experts.0.1.bias', 'flattenIntraCycleLayer.general_experts.0.2.weight', 'flattenIntraCycleLayer.general_experts.0.2.bias', 'flattenIntraCycleLayer.general_experts.1.1.weight', 'flattenIntraCycleLayer.general_experts.1.1.bias', 'flattenIntraCycleLayer.general_experts.1.2.weight', 'flattenIntraCycleLayer.general_experts.1.2.bias', 'flattenIntraCycleLayer.general_experts.2.1.weight', 'flattenIntraCycleLayer.general_experts.2.1.bias', 'flattenIntraCycleLayer.general_experts.2.2.weight', 'flattenIntraCycleLayer.general_experts.2.2.bias', 'flattenIntraCycleLayer.general_experts.3.1.weight', 'flattenIntraCycleLayer.general_experts.3.1.bias', 'flattenIntraCycleLayer.general_experts.3.2.weight', 'flattenIntraCycleLayer.general_experts.3.2.bias', 'intra_MoE_layers.0.view_experts.0.experts.0.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.0.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.0.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.1.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.1.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.1.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.2.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.2.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.2.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.3.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.3.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.3.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.4.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.4.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.4.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.5.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.5.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.5.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.6.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.6.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.6.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.7.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.7.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.7.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.8.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.8.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.8.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.9.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.9.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.9.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.10.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.10.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.10.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.11.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.11.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.11.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.12.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.12.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.12.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.13.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.13.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.13.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.14.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.14.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.14.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.15.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.15.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.15.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.16.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.16.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.16.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.17.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.17.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.17.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.18.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.18.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.18.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.19.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.19.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.19.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.20.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.20.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.20.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.21.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.21.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.21.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.22.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.22.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.22.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.23.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.23.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.23.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.24.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.24.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.24.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.25.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.25.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.25.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.26.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.26.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.26.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.27.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.27.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.27.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.28.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.28.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.28.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.29.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.29.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.29.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.30.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.30.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.30.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.31.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.31.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.31.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.32.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.32.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.32.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.33.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.33.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.33.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.34.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.34.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.34.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.35.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.35.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.35.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.36.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.36.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.36.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.37.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.37.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.37.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.38.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.38.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.38.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.39.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.39.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.39.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.40.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.40.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.40.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.41.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.41.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.41.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.42.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.42.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.42.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.43.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.43.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.43.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.44.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.44.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.44.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.45.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.45.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.45.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.46.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.46.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.46.out_linear.bias', 'intra_MoE_layers.0.view_experts.0.experts.47.in_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.47.out_linear.weight', 'intra_MoE_layers.0.view_experts.0.experts.47.out_linear.bias', 'intra_MoE_layers.0.general_experts.0.in_linear.weight', 'intra_MoE_layers.0.general_experts.0.out_linear.weight', 'intra_MoE_layers.0.general_experts.0.out_linear.bias', 'intra_MoE_layers.0.general_experts.1.in_linear.weight', 'intra_MoE_layers.0.general_experts.1.out_linear.weight', 'intra_MoE_layers.0.general_experts.1.out_linear.bias', 'intra_MoE_layers.0.general_experts.2.in_linear.weight', 'intra_MoE_layers.0.general_experts.2.out_linear.weight', 'intra_MoE_layers.0.general_experts.2.out_linear.bias', 'intra_MoE_layers.0.general_experts.3.in_linear.weight', 'intra_MoE_layers.0.general_experts.3.out_linear.weight', 'intra_MoE_layers.0.general_experts.3.out_linear.bias', 'intra_MoE_layers.0.norm.weight', 'intra_MoE_layers.0.norm.bias', 'flattenInterCycleLayer.view_experts.0.experts.0.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.0.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.0.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.0.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.1.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.1.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.1.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.1.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.2.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.2.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.2.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.2.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.3.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.3.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.3.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.3.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.4.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.4.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.4.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.4.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.5.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.5.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.5.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.5.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.6.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.6.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.6.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.6.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.7.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.7.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.7.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.7.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.8.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.8.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.8.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.8.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.9.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.9.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.9.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.9.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.10.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.10.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.10.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.10.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.11.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.11.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.11.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.11.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.12.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.12.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.12.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.12.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.13.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.13.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.13.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.13.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.14.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.14.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.14.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.14.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.15.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.15.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.15.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.15.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.16.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.16.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.16.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.16.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.17.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.17.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.17.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.17.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.18.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.18.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.18.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.18.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.19.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.19.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.19.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.19.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.20.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.20.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.20.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.20.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.21.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.21.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.21.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.21.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.22.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.22.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.22.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.22.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.23.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.23.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.23.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.23.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.24.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.24.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.24.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.24.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.25.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.25.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.25.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.25.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.26.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.26.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.26.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.26.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.27.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.27.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.27.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.27.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.28.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.28.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.28.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.28.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.29.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.29.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.29.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.29.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.30.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.30.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.30.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.30.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.31.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.31.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.31.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.31.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.32.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.32.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.32.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.32.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.33.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.33.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.33.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.33.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.34.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.34.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.34.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.34.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.35.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.35.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.35.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.35.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.36.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.36.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.36.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.36.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.37.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.37.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.37.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.37.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.38.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.38.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.38.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.38.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.39.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.39.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.39.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.39.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.40.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.40.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.40.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.40.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.41.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.41.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.41.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.41.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.42.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.42.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.42.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.42.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.43.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.43.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.43.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.43.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.44.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.44.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.44.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.44.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.45.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.45.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.45.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.45.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.46.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.46.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.46.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.46.2.bias', 'flattenInterCycleLayer.view_experts.0.experts.47.0.weight', 'flattenInterCycleLayer.view_experts.0.experts.47.0.bias', 'flattenInterCycleLayer.view_experts.0.experts.47.2.weight', 'flattenInterCycleLayer.view_experts.0.experts.47.2.bias', 'flattenInterCycleLayer.general_experts.0.0.weight', 'flattenInterCycleLayer.general_experts.0.0.bias', 'flattenInterCycleLayer.general_experts.0.2.weight', 'flattenInterCycleLayer.general_experts.0.2.bias', 'flattenInterCycleLayer.general_experts.1.0.weight', 'flattenInterCycleLayer.general_experts.1.0.bias', 'flattenInterCycleLayer.general_experts.1.2.weight', 'flattenInterCycleLayer.general_experts.1.2.bias', 'flattenInterCycleLayer.general_experts.2.0.weight', 'flattenInterCycleLayer.general_experts.2.0.bias', 'flattenInterCycleLayer.general_experts.2.2.weight', 'flattenInterCycleLayer.general_experts.2.2.bias', 'flattenInterCycleLayer.general_experts.3.0.weight', 'flattenInterCycleLayer.general_experts.3.0.bias', 'flattenInterCycleLayer.general_experts.3.2.weight', 'flattenInterCycleLayer.general_experts.3.2.bias', 'inter_MoE_layers.0.view_experts.0.experts.0.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.0.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.0.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.1.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.1.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.1.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.2.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.2.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.2.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.3.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.3.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.3.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.4.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.4.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.4.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.5.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.5.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.5.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.6.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.6.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.6.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.7.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.7.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.7.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.8.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.8.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.8.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.9.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.9.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.9.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.10.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.10.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.10.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.11.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.11.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.11.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.12.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.12.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.12.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.13.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.13.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.13.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.14.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.14.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.14.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.15.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.15.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.15.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.16.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.16.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.16.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.17.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.17.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.17.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.18.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.18.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.18.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.19.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.19.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.19.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.20.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.20.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.20.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.21.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.21.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.21.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.22.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.22.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.22.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.23.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.23.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.23.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.24.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.24.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.24.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.25.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.25.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.25.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.26.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.26.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.26.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.27.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.27.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.27.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.28.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.28.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.28.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.29.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.29.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.29.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.30.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.30.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.30.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.31.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.31.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.31.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.32.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.32.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.32.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.33.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.33.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.33.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.34.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.34.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.34.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.35.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.35.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.35.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.36.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.36.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.36.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.37.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.37.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.37.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.38.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.38.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.38.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.39.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.39.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.39.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.40.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.40.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.40.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.41.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.41.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.41.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.42.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.42.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.42.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.43.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.43.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.43.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.44.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.44.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.44.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.45.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.45.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.45.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.46.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.46.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.46.out_linear.bias', 'inter_MoE_layers.0.view_experts.0.experts.47.in_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.47.out_linear.weight', 'inter_MoE_layers.0.view_experts.0.experts.47.out_linear.bias', 'inter_MoE_layers.0.general_experts.0.in_linear.weight', 'inter_MoE_layers.0.general_experts.0.out_linear.weight', 'inter_MoE_layers.0.general_experts.0.out_linear.bias', 'inter_MoE_layers.0.general_experts.1.in_linear.weight', 'inter_MoE_layers.0.general_experts.1.out_linear.weight', 'inter_MoE_layers.0.general_experts.1.out_linear.bias', 'inter_MoE_layers.0.general_experts.2.in_linear.weight', 'inter_MoE_layers.0.general_experts.2.out_linear.weight', 'inter_MoE_layers.0.general_experts.2.out_linear.bias', 'inter_MoE_layers.0.general_experts.3.in_linear.weight', 'inter_MoE_layers.0.general_experts.3.out_linear.weight', 'inter_MoE_layers.0.general_experts.3.out_linear.bias', 'inter_MoE_layers.0.norm.weight', 'inter_MoE_layers.0.norm.bias', 'regression_head.projection.0.weight', 'regression_head.projection.0.bias']
[2025-04-18 15:21:39,477] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2025-04-18 15:21:39,477] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-04-18 15:21:39,669] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-04-18 15:21:39,670] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-04-18 15:21:39,670] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-18 15:21:39,703] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2025-04-18 15:21:39,703] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2025-04-18 15:21:39,703] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-04-18 15:21:39,703] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 200000000
[2025-04-18 15:21:39,703] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 200000000
[2025-04-18 15:21:39,703] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2025-04-18 15:21:39,703] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2025-04-18 15:21:39,989] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-18 15:21:39,990] [INFO] [utils.py:782:see_memory_usage] MA 0.06 GB         Max_MA 0.07 GB         CA 0.11 GB         Max_CA 0 GB 
[2025-04-18 15:21:39,991] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.99 GB, percent = 7.5%
[2025-04-18 15:21:40,167] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-18 15:21:40,168] [INFO] [utils.py:782:see_memory_usage] MA 0.06 GB         Max_MA 0.09 GB         CA 0.14 GB         Max_CA 0 GB 
[2025-04-18 15:21:40,168] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 19.07 GB, percent = 7.6%
[2025-04-18 15:21:40,168] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2025-04-18 15:21:40,374] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-18 15:21:40,375] [INFO] [utils.py:782:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 0.14 GB         Max_CA 0 GB 
[2025-04-18 15:21:40,375] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 19.33 GB, percent = 7.7%
[2025-04-18 15:21:40,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-18 15:21:40,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-04-18 15:21:40,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-04-18 15:21:40,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]
[2025-04-18 15:21:40,382] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-04-18 15:21:40,382] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-18 15:21:40,382] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3ad6e02f10>
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-04-18 15:21:40,383] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2025-04-18 15:21:40,384] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-18 15:21:40,385] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   train_batch_size ............. 116
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  58
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   world_size ................... 2
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-18 15:21:40,386] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2
[2025-04-18 15:21:40,386] [INFO] [config.py:989:print_user_config]   json = {
    "bf16": {
        "enabled": true, 
        "auto_cast": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09
    }, 
    "gradient_accumulation_steps": 1, 
    "train_batch_size": 116, 
    "train_micro_batch_size_per_gpu": 58, 
    "steps_per_print": inf, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Epoch: 1 cost time: 2.062915325164795
Epoch: 1 | Train Loss: 1.09300 | Train label loss: 1.09300 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 236.2898238 | Train MAPE: 0.3850592 | Vali RMSE: 1624.8237320| Vali MAE: 1332.7401120| Vali MAPE: 0.5768142| Test RMSE: 912.1529480| Test acc1: 0.0000 | Test MAPE: 0.6795538
The checkpoint is saved in /data/LLMs/checkpoints/BatteryMoE_bottleneck_all_sl1_lr5e-05_dm128_nh8_el1_dl1_df128_llmLayers32_lradjtype1_datasetexp_guideFalse_LBFalse_lossMSE_wd0.0_wlFalse_noDKPL1_dr0.05_bf8_NumE20_K-1_seed2021-50to1!
type1| Updating learning rate to 5e-05
Epoch: 2 cost time: 1.6856343746185303
Epoch: 2 | Train Loss: 0.46540 | Train label loss: 0.46540 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 159.1459532 | Train MAPE: 0.2905903 | Vali RMSE: 1613.6466153| Vali MAE: 1371.3951224| Vali MAPE: 0.8686846| Test RMSE: 876.8979835| Test acc1: 15.6667 | Test MAPE: 0.4039738
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 3 cost time: 1.5690038204193115
Epoch: 3 | Train Loss: 0.26280 | Train label loss: 0.26280 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 122.9327092 | Train MAPE: 0.2181762 | Vali RMSE: 1564.8060988| Vali MAE: 1373.4718195| Vali MAPE: 1.1349421| Test RMSE: 832.0984049| Test acc1: 57.6667 | Test MAPE: 0.3076194
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 4 cost time: 1.7278337478637695
Epoch: 4 | Train Loss: 0.21501 | Train label loss: 0.21501 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 107.8119996 | Train MAPE: 0.1858355 | Vali RMSE: 1660.8414679| Vali MAE: 1418.3981972| Vali MAPE: 0.9377043| Test RMSE: 902.9142164| Test acc1: 16.0000 | Test MAPE: 0.3884471
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 5 cost time: 1.8409795761108398
Epoch: 5 | Train Loss: 0.22094 | Train label loss: 0.22094 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 109.3535955 | Train MAPE: 0.1303379 | Vali RMSE: 1675.5100668| Vali MAE: 1427.1934347| Vali MAPE: 0.9219589| Test RMSE: 913.7070668| Test acc1: 8.3333 | Test MAPE: 0.4270572
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 6 cost time: 1.8692984580993652
Epoch: 6 | Train Loss: 0.16241 | Train label loss: 0.16241 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 95.8315002 | Train MAPE: 0.1221842 | Vali RMSE: 1645.3771992| Vali MAE: 1420.5455652| Vali MAPE: 1.0307863| Test RMSE: 887.7436242| Test acc1: 37.0000 | Test MAPE: 0.3493905
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 7 cost time: 1.658010721206665
Epoch: 7 | Train Loss: 0.16810 | Train label loss: 0.16810 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 98.3598222 | Train MAPE: 0.1303130 | Vali RMSE: 1675.9081340| Vali MAE: 1435.8741843| Vali MAPE: 0.9758732| Test RMSE: 912.7562911| Test acc1: 25.0000 | Test MAPE: 0.4162929
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 8 cost time: 1.7924237251281738
Epoch: 8 | Train Loss: 0.15723 | Train label loss: 0.15723 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 91.1290405 | Train MAPE: 0.1110790 | Vali RMSE: 1703.5238619| Vali MAE: 1449.9496588| Vali MAPE: 0.9288908| Test RMSE: 933.6582723| Test acc1: 0.0000 | Test MAPE: 0.4613664
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 9 cost time: 1.8233675956726074
Epoch: 9 | Train Loss: 0.15830 | Train label loss: 0.15830 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 94.2968222 | Train MAPE: 0.1047661 | Vali RMSE: 1678.0812828| Vali MAE: 1440.8171559| Vali MAPE: 0.9969689| Test RMSE: 912.0697856| Test acc1: 33.3333 | Test MAPE: 0.3884941
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 10 cost time: 1.7995624542236328
Epoch: 10 | Train Loss: 0.15237 | Train label loss: 0.15237 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 93.4687023 | Train MAPE: 0.1161629 | Vali RMSE: 1668.8718709| Vali MAE: 1436.1170792| Vali MAPE: 1.0128127| Test RMSE: 904.6432749| Test acc1: 33.3333 | Test MAPE: 0.3589433
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 11 cost time: 1.7054786682128906
Epoch: 11 | Train Loss: 0.14791 | Train label loss: 0.14791 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 88.9317741 | Train MAPE: 0.1059548 | Vali RMSE: 1699.1910348| Vali MAE: 1446.8895043| Vali MAPE: 0.9305260| Test RMSE: 929.3610899| Test acc1: 1.0000 | Test MAPE: 0.4239511
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 12 cost time: 1.711975336074829
Epoch: 12 | Train Loss: 0.16083 | Train label loss: 0.16083 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 90.7103230 | Train MAPE: 0.1012616 | Vali RMSE: 1706.0319638| Vali MAE: 1449.5195657| Vali MAPE: 0.9135967| Test RMSE: 935.0041323| Test acc1: 0.0000 | Test MAPE: 0.4378743
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 13 cost time: 1.683403730392456
Epoch: 13 | Train Loss: 0.12962 | Train label loss: 0.12962 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 87.2674914 | Train MAPE: 0.0939888 | Vali RMSE: 1683.1480631| Vali MAE: 1442.2404830| Vali MAPE: 0.9807135| Test RMSE: 915.6046225| Test acc1: 33.0000 | Test MAPE: 0.3835117
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 14 cost time: 1.851696491241455
Epoch: 14 | Train Loss: 0.13524 | Train label loss: 0.13524 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 87.6858479 | Train MAPE: 0.1018981 | Vali RMSE: 1686.0308217| Vali MAE: 1443.8154255| Vali MAPE: 0.9765474| Test RMSE: 918.0168651| Test acc1: 32.0000 | Test MAPE: 0.3922035
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 15 cost time: 1.5828571319580078
Epoch: 15 | Train Loss: 0.14317 | Train label loss: 0.14317 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 86.0011362 | Train MAPE: 0.0882309 | Vali RMSE: 1697.4945095| Vali MAE: 1448.5944027| Vali MAPE: 0.9502345| Test RMSE: 927.4891925| Test acc1: 6.3333 | Test MAPE: 0.4198378
EarlyStopping counter: 0 out of 5
type1| Updating learning rate to 5e-05
Epoch: 16 cost time: 1.5680744647979736
Epoch: 16 | Train Loss: 0.12836 | Train label loss: 0.12836 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 82.4008482 | Train MAPE: 0.0852805 | Vali RMSE: 1694.8676599| Vali MAE: 1447.3257854| Vali MAPE: 0.9551831| Test RMSE: 926.3332371| Test acc1: 10.3333 | Test MAPE: 0.4158228
EarlyStopping counter: 1 out of 5
type1| Updating learning rate to 2.5e-05
Epoch: 17 cost time: 1.7523441314697266
Epoch: 17 | Train Loss: 0.12011 | Train label loss: 0.12011 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 82.3024572 | Train MAPE: 0.0874228 | Vali RMSE: 1691.8156939| Vali MAE: 1446.3263621| Vali MAPE: 0.9640115| Test RMSE: 923.7845095| Test acc1: 24.0000 | Test MAPE: 0.4065956
EarlyStopping counter: 2 out of 5
type1| Updating learning rate to 1.25e-05
Epoch: 18 cost time: 1.7824468612670898
Epoch: 18 | Train Loss: 0.11788 | Train label loss: 0.11788 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 79.5660533 | Train MAPE: 0.0806421 | Vali RMSE: 1693.6444757| Vali MAE: 1447.4371769| Vali MAPE: 0.9621170| Test RMSE: 925.5428089| Test acc1: 21.0000 | Test MAPE: 0.4068205
EarlyStopping counter: 3 out of 5
type1| Updating learning rate to 6.25e-06
Epoch: 19 cost time: 1.8168537616729736
Epoch: 19 | Train Loss: 0.10437 | Train label loss: 0.10437 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 76.8701864 | Train MAPE: 0.0802752 | Vali RMSE: 1692.8007869| Vali MAE: 1447.4743063| Vali MAPE: 0.9665543| Test RMSE: 924.5724135| Test acc1: 25.0000 | Test MAPE: 0.4028166
EarlyStopping counter: 4 out of 5
type1| Updating learning rate to 3.125e-06
Epoch: 20 cost time: 1.8481926918029785
Epoch: 20 | Train Loss: 0.10656 | Train label loss: 0.10656 | Train cl loss: 0.00000| Train align2 loss: 0.00000 | Train align loss: 0.00000 | Train DG loss: 0.00000 | Train RMSE: 75.5060996 | Train MAPE: 0.0813264 | Vali RMSE: 1696.0212923| Vali MAE: 1449.2565602| Vali MAPE: 0.9620632| Test RMSE: 927.1174576| Test acc1: 20.6667 | Test MAPE: 0.4081908
EarlyStopping counter: 5 out of 5
Early stopping
Best model performance: Test MAE: 687.5496 | Test RMSE: 912.1529 | Test MAPE: 0.6796 | Test 15%-accuracy: 0.0000 | Test 10%-accuracy: 0.0000 | Val MAE: 1332.7401 | Val RMSE: 1624.8237 | Val MAPE: 0.5768 | Val 15%-accuracy: 21.6667 | Val 10%-accuracy: 18.6667 
Best model performance: Test Seen MAPE: 0.6404 | Test Unseen MAPE: 0.7579
Best model performance: Test Seen 15%-accuracy: 0.0000 | Test Unseen 15%-accuracy: 0.0000
Best model performance: Test Seen 10%-accuracy: 0.0000 | Test Unseen 10%-accuracy: 0.0000
/data/LLMs/checkpoints/BatteryMoE_bottleneck_all_sl1_lr5e-05_dm128_nh8_el1_dl1_df128_llmLayers32_lradjtype1_datasetexp_guideFalse_LBFalse_lossMSE_wd0.0_wlFalse_noDKPL1_dr0.05_bf8_NumE20_K-1_seed2021-50to1
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.030 MB of 0.173 MB uploadedwandb: / 0.177 MB of 0.177 MB uploadedwandb: - 0.177 MB of 0.177 MB uploadedwandb: \ 0.177 MB of 0.177 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  test_MAPE â–ˆâ–ƒâ–â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆ
wandb:  test_RMSE â–†â–„â–â–†â–‡â–…â–†â–ˆâ–†â–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†
wandb:  test_acc1 â–â–ƒâ–ˆâ–ƒâ–‚â–…â–„â–â–…â–…â–â–â–…â–…â–‚â–‚â–„â–„â–„â–„â–
wandb:  test_acc2 â–â–ƒâ–ˆâ–â–â–ˆâ–‚â–â–…â–ˆâ–â–â–â–â–â–â–â–â–â–â–
wandb: train_loss â–ˆâ–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  vali_MAPE â–â–…â–ˆâ–†â–…â–‡â–†â–…â–†â–†â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–
wandb:  vali_RMSE â–„â–ƒâ–â–†â–†â–…â–‡â–ˆâ–‡â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–„
wandb:  vali_acc1 â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:  vali_acc2 â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb: 
wandb: Run summary:
wandb:      epoch 20
wandb:  test_MAPE 0.67955
wandb:  test_RMSE 912.15295
wandb:  test_acc1 0.0
wandb:  test_acc2 0.0
wandb: train_loss 0.10656
wandb:  vali_MAPE 0.57681
wandb:  vali_RMSE 1624.82373
wandb:  vali_acc1 21.66667
wandb:  vali_acc2 18.66667
wandb: 
wandb: ðŸš€ View run 2025-04-18 15:21:24 at: https://wandb.ai/hkustgz-batteryinformatics/BatteryMoE_FixPrompt/runs/4objvd75
wandb: â­ï¸ View project at: https://wandb.ai/hkustgz-batteryinformatics/BatteryMoE_FixPrompt
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250418_152135-4objvd75/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[rank0]:[W418 15:23:20.581242325 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
