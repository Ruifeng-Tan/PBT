{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "This notebook is used to obtain the embeddings of the domain-knowledge prompt for pretraining PBT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import transformers\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import json\n",
    "import torch\n",
    "from data_provider.data_split_recorder import split_recorder\n",
    "from Prompts.Mapping_helper import Mapping_helper\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, AutoTokenizer, AutoModel, AutoConfig, Phi3Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ax_linewidth(ax, bw=1.5):\n",
    "    ax.spines['bottom'].set_linewidth(bw)\n",
    "    ax.spines['left'].set_linewidth(bw)\n",
    "    ax.spines['top'].set_linewidth(bw)\n",
    "    ax.spines['right'].set_linewidth(bw)\n",
    "\n",
    "def set_ax_font_size(ax, fontsize=10):\n",
    "    ax.tick_params(axis='y',\n",
    "                 labelsize=fontsize # y轴字体大小设置\n",
    "                  ) \n",
    "    ax.tick_params(axis='x',\n",
    "                 labelsize=fontsize # x轴字体大小设置\n",
    "                  ) \n",
    "\n",
    "def set_draft(the_plt, other_ax=''):\n",
    "    ax = the_plt.gca()\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    if other_ax:\n",
    "        other_ax.axes.xaxis.set_ticklabels([])\n",
    "        other_ax.axes.yaxis.set_ticklabels([])\n",
    "        other_ax.set_ylabel('')\n",
    "        other_ax.set_xlabel('')\n",
    "\n",
    "def set_draft_fig(fig):\n",
    "    for ax in fig.axes:\n",
    "        ax.axes.xaxis.set_ticklabels([])\n",
    "        ax.axes.yaxis.set_ticklabels([])\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UL-PUR_N10-NA7_18650_NCA_23C_0-100_0.5-0.5C_g',\n",
       " 'UL-PUR_N15-NA10_18650_NCA_23C_0-100_0.5-0.5C_j',\n",
       " 'RWTH_016',\n",
       " 'RWTH_045',\n",
       " 'RWTH_009',\n",
       " 'RWTH_039',\n",
       " 'RWTH_046',\n",
       " 'RWTH_019',\n",
       " 'RWTH_037',\n",
       " 'RWTH_013',\n",
       " 'RWTH_003',\n",
       " 'RWTH_044',\n",
       " 'RWTH_026',\n",
       " 'RWTH_006',\n",
       " 'RWTH_031',\n",
       " 'RWTH_036',\n",
       " 'RWTH_048',\n",
       " 'RWTH_033',\n",
       " 'RWTH_021',\n",
       " 'RWTH_012',\n",
       " 'RWTH_034',\n",
       " 'RWTH_018',\n",
       " 'RWTH_022',\n",
       " 'RWTH_030',\n",
       " 'RWTH_028',\n",
       " 'RWTH_011',\n",
       " 'RWTH_040',\n",
       " 'RWTH_041',\n",
       " 'RWTH_042',\n",
       " 'RWTH_025',\n",
       " 'RWTH_047',\n",
       " 'RWTH_004',\n",
       " 'HUST_1-6',\n",
       " 'HUST_2-2',\n",
       " 'HUST_1-3',\n",
       " 'HUST_6-3',\n",
       " 'HUST_1-2',\n",
       " 'HUST_3-7',\n",
       " 'HUST_3-2',\n",
       " 'HUST_10-6',\n",
       " 'HUST_3-6',\n",
       " 'HUST_5-1',\n",
       " 'HUST_10-5',\n",
       " 'HUST_6-2',\n",
       " 'HUST_6-1',\n",
       " 'HUST_8-1',\n",
       " 'HUST_10-7',\n",
       " 'HUST_1-4',\n",
       " 'HUST_5-4',\n",
       " 'HUST_1-5',\n",
       " 'HUST_6-6',\n",
       " 'HUST_5-6',\n",
       " 'HUST_6-4',\n",
       " 'HUST_9-2',\n",
       " 'HUST_10-4',\n",
       " 'HUST_5-3',\n",
       " 'HUST_7-7',\n",
       " 'HUST_3-1',\n",
       " 'HUST_4-1',\n",
       " 'HUST_4-4',\n",
       " 'HUST_4-6',\n",
       " 'HUST_8-8',\n",
       " 'HUST_2-4',\n",
       " 'HUST_9-8',\n",
       " 'HUST_9-5',\n",
       " 'HUST_3-3',\n",
       " 'HUST_1-7',\n",
       " 'HUST_4-5',\n",
       " 'HUST_9-6',\n",
       " 'HUST_1-1',\n",
       " 'HUST_4-3',\n",
       " 'HUST_2-5',\n",
       " 'HUST_4-7',\n",
       " 'HUST_7-2',\n",
       " 'HUST_8-4',\n",
       " 'HUST_3-5',\n",
       " 'HUST_2-6',\n",
       " 'HUST_8-6',\n",
       " 'HUST_7-5',\n",
       " 'MATR_b1c5',\n",
       " 'MATR_b3c6',\n",
       " 'MATR_b1c7',\n",
       " 'MATR_b4c18',\n",
       " 'MATR_b3c44',\n",
       " 'MATR_b3c18',\n",
       " 'MATR_b2c12',\n",
       " 'MATR_b2c26',\n",
       " 'MATR_b4c26',\n",
       " 'MATR_b3c40',\n",
       " 'MATR_b1c9',\n",
       " 'MATR_b2c18',\n",
       " 'MATR_b3c17',\n",
       " 'MATR_b3c21',\n",
       " 'MATR_b2c6',\n",
       " 'MATR_b3c45',\n",
       " 'MATR_b1c23',\n",
       " 'MATR_b2c4',\n",
       " 'MATR_b1c35',\n",
       " 'MATR_b1c19',\n",
       " 'MATR_b2c39',\n",
       " 'MATR_b3c26',\n",
       " 'MATR_b2c28',\n",
       " 'MATR_b2c33',\n",
       " 'MATR_b3c39',\n",
       " 'MATR_b2c29',\n",
       " 'MATR_b3c31',\n",
       " 'MATR_b4c42',\n",
       " 'MATR_b4c7',\n",
       " 'MATR_b2c38',\n",
       " 'MATR_b1c41',\n",
       " 'MATR_b3c33',\n",
       " 'MATR_b4c40',\n",
       " 'MATR_b1c17',\n",
       " 'MATR_b3c1',\n",
       " 'MATR_b4c10',\n",
       " 'MATR_b1c33',\n",
       " 'MATR_b2c10',\n",
       " 'MATR_b3c13',\n",
       " 'MATR_b4c37',\n",
       " 'MATR_b4c23',\n",
       " 'MATR_b4c15',\n",
       " 'MATR_b2c0',\n",
       " 'MATR_b2c19',\n",
       " 'MATR_b4c1',\n",
       " 'MATR_b3c8',\n",
       " 'MATR_b1c15',\n",
       " 'MATR_b4c24',\n",
       " 'MATR_b3c15',\n",
       " 'MATR_b1c3',\n",
       " 'MATR_b1c16',\n",
       " 'MATR_b3c3',\n",
       " 'MATR_b4c20',\n",
       " 'MATR_b4c30',\n",
       " 'MATR_b4c25',\n",
       " 'MATR_b4c9',\n",
       " 'MATR_b1c20',\n",
       " 'MATR_b3c14',\n",
       " 'MATR_b2c5',\n",
       " 'MATR_b3c22',\n",
       " 'MATR_b3c16',\n",
       " 'MATR_b4c43',\n",
       " 'MATR_b4c19',\n",
       " 'MATR_b2c31',\n",
       " 'MATR_b2c21',\n",
       " 'MATR_b4c12',\n",
       " 'MATR_b2c36',\n",
       " 'MATR_b1c21',\n",
       " 'MATR_b2c3',\n",
       " 'MATR_b2c37',\n",
       " 'MATR_b4c4',\n",
       " 'MATR_b2c44',\n",
       " 'MATR_b4c34',\n",
       " 'MATR_b4c29',\n",
       " 'MATR_b3c7',\n",
       " 'MATR_b4c21',\n",
       " 'MATR_b2c1',\n",
       " 'MATR_b1c31',\n",
       " 'MATR_b2c14',\n",
       " 'MATR_b1c26',\n",
       " 'MATR_b4c38',\n",
       " 'MATR_b1c42',\n",
       " 'MATR_b2c17',\n",
       " 'MATR_b3c28',\n",
       " 'MATR_b3c10',\n",
       " 'MATR_b3c36',\n",
       " 'MATR_b3c24',\n",
       " 'MATR_b1c6',\n",
       " 'MATR_b2c34',\n",
       " 'MATR_b3c9',\n",
       " 'MATR_b4c14',\n",
       " 'MATR_b2c24',\n",
       " 'MATR_b2c30',\n",
       " 'MATR_b3c4',\n",
       " 'MATR_b4c11',\n",
       " 'MATR_b2c41',\n",
       " 'MATR_b4c8',\n",
       " 'MATR_b3c25',\n",
       " 'MATR_b1c38',\n",
       " 'MATR_b3c27',\n",
       " 'MATR_b1c18',\n",
       " 'MATR_b2c32',\n",
       " 'Stanford_Nova_Regular_228',\n",
       " 'Stanford_Nova_Regular_203',\n",
       " 'Stanford_Nova_Regular_215',\n",
       " 'Stanford_Nova_Regular_225',\n",
       " 'Stanford_Nova_Regular_222',\n",
       " 'Stanford_Nova_Regular_226',\n",
       " 'Stanford_Nova_Regular_221',\n",
       " 'Stanford_Nova_Regular_211',\n",
       " 'Stanford_Nova_Regular_219',\n",
       " 'Stanford_Nova_Regular_229',\n",
       " 'Stanford_Nova_Regular_193',\n",
       " 'Stanford_Nova_Regular_200',\n",
       " 'Stanford_Nova_Regular_205',\n",
       " 'Stanford_Nova_Regular_230',\n",
       " 'Stanford_Nova_Regular_196',\n",
       " 'Stanford_Nova_Regular_216',\n",
       " 'Stanford_Nova_Regular_220',\n",
       " 'Stanford_Nova_Regular_201',\n",
       " 'Stanford_Nova_Regular_Ref_101',\n",
       " 'Stanford_Nova_Regular_192',\n",
       " 'Stanford_Nova_Regular_208',\n",
       " 'Stanford_Nova_Regular_Ref_102',\n",
       " 'Stanford_Nova_Regular_210',\n",
       " 'Stanford_Nova_Regular_212',\n",
       " 'Stanford_Nova_Regular_199',\n",
       " 'Tongji1_CY35-05_1--2',\n",
       " 'Tongji1_CY45-05_1--21',\n",
       " 'Tongji1_CY45-05_1--28',\n",
       " 'Tongji2_CY45-05_1--11',\n",
       " 'Tongji1_CY25-1_1--5',\n",
       " 'Tongji1_CY25-1_1--9',\n",
       " 'Tongji1_CY45-05_1--19',\n",
       " 'Tongji3_CY25-05_4--1',\n",
       " 'Tongji1_CY25-05_1--2',\n",
       " 'Tongji3_CY25-05_2--1',\n",
       " 'Tongji1_CY45-05_1--5',\n",
       " 'Tongji1_CY45-05_1--13',\n",
       " 'Tongji1_CY45-05_1--11',\n",
       " 'Tongji1_CY35-05_1--1',\n",
       " 'Tongji1_CY45-05_1--17',\n",
       " 'Tongji2_CY25-05_1--13',\n",
       " 'Tongji1_CY25-1_1--6',\n",
       " 'Tongji2_CY45-05_1--22',\n",
       " 'Tongji1_CY45-05_1--27',\n",
       " 'Tongji2_CY45-05_1--27',\n",
       " 'Tongji1_CY25-05_1--17',\n",
       " 'Tongji2_CY45-05_1--9',\n",
       " 'Tongji3_CY25-05_2--2',\n",
       " 'Tongji1_CY45-05_1--18',\n",
       " 'Tongji3_CY25-05_4--2',\n",
       " 'Tongji1_CY45-05_1--20',\n",
       " 'Tongji1_CY45-05_1--1',\n",
       " 'Tongji2_CY45-05_1--23',\n",
       " 'Tongji2_CY25-05_1--5',\n",
       " 'Tongji2_CY45-05_1--25',\n",
       " 'Tongji1_CY25-05_1--4',\n",
       " 'Tongji1_CY25-05_1--12',\n",
       " 'Tongji1_CY25-05_1--16',\n",
       " 'Tongji1_CY25-05_1--13',\n",
       " 'Tongji2_CY25-05_1--17',\n",
       " 'Tongji1_CY45-05_1--7',\n",
       " 'Tongji1_CY25-025_1--5',\n",
       " 'Tongji2_CY25-05_1--12',\n",
       " 'Tongji1_CY45-05_1--6',\n",
       " 'Tongji3_CY25-05_1--2',\n",
       " 'Tongji3_CY25-05_2--3',\n",
       " 'Tongji2_CY45-05_1--16',\n",
       " 'Tongji1_CY25-05_1--15',\n",
       " 'Tongji1_CY25-025_1--3',\n",
       " 'Tongji2_CY45-05_1--7',\n",
       " 'Tongji2_CY45-05_1--20',\n",
       " 'Tongji1_CY45-05_1--23',\n",
       " 'Tongji2_CY25-05_1--15',\n",
       " 'Tongji2_CY35-05_1--4',\n",
       " 'Tongji1_CY45-05_1--12',\n",
       " 'Tongji2_CY45-05_1--15',\n",
       " 'Tongji1_CY25-05_1--5',\n",
       " 'Tongji2_CY35-05_1--3',\n",
       " 'Tongji1_CY25-05_1--14',\n",
       " 'Tongji1_CY45-05_1--26',\n",
       " 'Tongji1_CY45-05_1--22',\n",
       " 'Tongji2_CY45-05_1--24',\n",
       " 'Tongji1_CY25-05_1--18',\n",
       " 'Tongji1_CY45-05_1--9',\n",
       " 'Tongji1_CY45-05_1--24',\n",
       " 'Tongji1_CY25-1_1--4',\n",
       " 'Tongji2_CY45-05_1--26',\n",
       " 'Tongji3_CY25-05_1--3',\n",
       " 'Tongji1_CY45-05_1--15',\n",
       " 'Tongji1_CY25-025_1--2',\n",
       " 'Tongji2_CY45-05_1--14',\n",
       " 'ISU-ILCC_G23C3',\n",
       " 'ISU-ILCC_G64C4',\n",
       " 'ISU-ILCC_G50C1',\n",
       " 'ISU-ILCC_G36C2',\n",
       " 'ISU-ILCC_G7C4',\n",
       " 'ISU-ILCC_G16C2',\n",
       " 'ISU-ILCC_G13C2',\n",
       " 'ISU-ILCC_G29C3',\n",
       " 'ISU-ILCC_G41C3',\n",
       " 'ISU-ILCC_G39C4',\n",
       " 'ISU-ILCC_G36C4',\n",
       " 'ISU-ILCC_G28C3',\n",
       " 'ISU-ILCC_G18C1',\n",
       " 'ISU-ILCC_G58C1',\n",
       " 'ISU-ILCC_G8C2',\n",
       " 'ISU-ILCC_G16C3',\n",
       " 'ISU-ILCC_G27C2',\n",
       " 'ISU-ILCC_G22C1',\n",
       " 'ISU-ILCC_G43C1',\n",
       " 'ISU-ILCC_G53C2',\n",
       " 'ISU-ILCC_G32C4',\n",
       " 'ISU-ILCC_G44C3',\n",
       " 'ISU-ILCC_G38C1',\n",
       " 'ISU-ILCC_G61C3',\n",
       " 'ISU-ILCC_G43C2',\n",
       " 'ISU-ILCC_G20C2',\n",
       " 'ISU-ILCC_G17C1',\n",
       " 'ISU-ILCC_G33C4',\n",
       " 'ISU-ILCC_G60C1',\n",
       " 'ISU-ILCC_G27C3',\n",
       " 'ISU-ILCC_G45C3',\n",
       " 'ISU-ILCC_G18C4',\n",
       " 'ISU-ILCC_G50C4',\n",
       " 'ISU-ILCC_G1C1',\n",
       " 'ISU-ILCC_G19C4',\n",
       " 'ISU-ILCC_G49C4',\n",
       " 'ISU-ILCC_G3C3',\n",
       " 'ISU-ILCC_G19C2',\n",
       " 'ISU-ILCC_G52C4',\n",
       " 'ISU-ILCC_G31C1',\n",
       " 'ISU-ILCC_G47C2',\n",
       " 'ISU-ILCC_G43C4',\n",
       " 'ISU-ILCC_G47C1',\n",
       " 'ISU-ILCC_G40C2',\n",
       " 'ISU-ILCC_G20C4',\n",
       " 'ISU-ILCC_G6C2',\n",
       " 'ISU-ILCC_G32C3',\n",
       " 'ISU-ILCC_G41C4',\n",
       " 'ISU-ILCC_G60C2',\n",
       " 'ISU-ILCC_G64C3',\n",
       " 'ISU-ILCC_G9C2',\n",
       " 'ISU-ILCC_G49C3',\n",
       " 'ISU-ILCC_G46C1',\n",
       " 'ISU-ILCC_G30C1',\n",
       " 'ISU-ILCC_G29C4',\n",
       " 'ISU-ILCC_G50C2',\n",
       " 'ISU-ILCC_G51C1',\n",
       " 'ISU-ILCC_G55C2',\n",
       " 'ISU-ILCC_G17C4',\n",
       " 'ISU-ILCC_G50C3',\n",
       " 'ISU-ILCC_G25C1',\n",
       " 'ISU-ILCC_G4C4',\n",
       " 'ISU-ILCC_G13C4',\n",
       " 'ISU-ILCC_G46C3',\n",
       " 'ISU-ILCC_G38C2',\n",
       " 'ISU-ILCC_G34C1',\n",
       " 'ISU-ILCC_G45C4',\n",
       " 'ISU-ILCC_G27C4',\n",
       " 'ISU-ILCC_G35C1',\n",
       " 'ISU-ILCC_G21C4',\n",
       " 'ISU-ILCC_G24C1',\n",
       " 'ISU-ILCC_G40C4',\n",
       " 'ISU-ILCC_G42C3',\n",
       " 'ISU-ILCC_G39C1',\n",
       " 'ISU-ILCC_G14C1',\n",
       " 'ISU-ILCC_G61C2',\n",
       " 'ISU-ILCC_G63C3',\n",
       " 'ISU-ILCC_G23C4',\n",
       " 'ISU-ILCC_G36C1',\n",
       " 'ISU-ILCC_G57C2',\n",
       " 'ISU-ILCC_G55C1',\n",
       " 'ISU-ILCC_G4C1',\n",
       " 'ISU-ILCC_G7C3',\n",
       " 'ISU-ILCC_G12C4',\n",
       " 'ISU-ILCC_G34C2',\n",
       " 'ISU-ILCC_G7C1',\n",
       " 'ISU-ILCC_G51C4',\n",
       " 'ISU-ILCC_G4C2',\n",
       " 'ISU-ILCC_G44C1',\n",
       " 'ISU-ILCC_G46C4',\n",
       " 'ISU-ILCC_G42C2',\n",
       " 'ISU-ILCC_G28C2',\n",
       " 'ISU-ILCC_G12C3',\n",
       " 'ISU-ILCC_G51C2',\n",
       " 'ISU-ILCC_G62C4',\n",
       " 'ISU-ILCC_G30C2',\n",
       " 'ISU-ILCC_G28C1',\n",
       " 'ISU-ILCC_G56C2',\n",
       " 'ISU-ILCC_G23C2',\n",
       " 'ISU-ILCC_G52C1',\n",
       " 'ISU-ILCC_G37C3',\n",
       " 'ISU-ILCC_G34C3',\n",
       " 'ISU-ILCC_G57C3',\n",
       " 'ISU-ILCC_G32C2',\n",
       " 'ISU-ILCC_G38C3',\n",
       " 'ISU-ILCC_G18C3',\n",
       " 'ISU-ILCC_G10C3',\n",
       " 'ISU-ILCC_G40C3',\n",
       " 'ISU-ILCC_G14C3',\n",
       " 'ISU-ILCC_G1C4',\n",
       " 'ISU-ILCC_G29C2',\n",
       " 'ISU-ILCC_G17C3',\n",
       " 'ISU-ILCC_G45C1',\n",
       " 'ISU-ILCC_G62C3',\n",
       " 'ISU-ILCC_G31C3',\n",
       " 'ISU-ILCC_G25C3',\n",
       " 'ISU-ILCC_G44C4',\n",
       " 'ISU-ILCC_G4C3',\n",
       " 'ISU-ILCC_G44C2',\n",
       " 'ISU-ILCC_G62C1',\n",
       " 'ISU-ILCC_G30C3',\n",
       " 'ISU-ILCC_G56C3',\n",
       " 'ISU-ILCC_G47C4',\n",
       " 'ISU-ILCC_G25C2',\n",
       " 'ISU-ILCC_G42C1',\n",
       " 'ISU-ILCC_G20C1',\n",
       " 'ISU-ILCC_G59C1',\n",
       " 'ISU-ILCC_G38C4',\n",
       " 'ISU-ILCC_G9C3',\n",
       " 'ISU-ILCC_G35C3',\n",
       " 'ISU-ILCC_G8C4',\n",
       " 'ISU-ILCC_G53C1',\n",
       " 'ISU-ILCC_G30C4',\n",
       " 'ISU-ILCC_G20C3',\n",
       " 'ISU-ILCC_G9C1',\n",
       " 'ISU-ILCC_G21C1',\n",
       " 'ISU-ILCC_G3C2',\n",
       " 'ISU-ILCC_G5C1',\n",
       " 'ISU-ILCC_G1C3',\n",
       " 'ISU-ILCC_G62C2',\n",
       " 'ISU-ILCC_G58C3',\n",
       " 'ISU-ILCC_G21C2',\n",
       " 'ISU-ILCC_G28C4',\n",
       " 'ISU-ILCC_G24C2',\n",
       " 'CALCE_CS2_36',\n",
       " 'CALCE_CS2_37',\n",
       " 'CALCE_CX2_33',\n",
       " 'CALCE_CS2_34',\n",
       " 'CALCE_CX2_37',\n",
       " 'CALCE_CS2_33',\n",
       " 'CALCE_CS2_35',\n",
       " 'CALCE_CS2_38',\n",
       " 'CALCE_CX2_36',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_o',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_p',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_e',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_l',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_b',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_t',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_c',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_a',\n",
       " 'HNEI_18650_NMC_LCO_25C_0-100_0.5-1.5C_g',\n",
       " 'SNL_18650_NCA_25C_0-100_0.5-0.5C_b',\n",
       " 'SNL_18650_NCA_35C_0-100_0.5-1C_b',\n",
       " 'SNL_18650_NMC_35C_0-100_0.5-2C_b',\n",
       " 'SNL_18650_NMC_25C_0-100_0.5-3C_b',\n",
       " 'SNL_18650_NMC_25C_0-100_0.5-1C_a',\n",
       " 'SNL_18650_LFP_25C_0-100_0.5-3C_c',\n",
       " 'SNL_18650_NCA_25C_0-100_0.5-2C_b',\n",
       " 'SNL_18650_NMC_15C_0-100_0.5-2C_b',\n",
       " 'SNL_18650_LFP_35C_0-100_0.5-1C_d',\n",
       " 'SNL_18650_NCA_35C_0-100_0.5-1C_a',\n",
       " 'SNL_18650_LFP_35C_0-100_0.5-2C_b',\n",
       " 'SNL_18650_NMC_25C_0-100_0.5-1C_c',\n",
       " 'SNL_18650_NMC_25C_0-100_0.5-3C_a',\n",
       " 'SNL_18650_NCA_35C_0-100_0.5-1C_d',\n",
       " 'SNL_18650_NMC_25C_0-100_0.5-1C_d',\n",
       " 'SNL_18650_LFP_25C_0-100_0.5-3C_a',\n",
       " 'SNL_18650_NCA_15C_0-100_0.5-2C_a',\n",
       " 'SNL_18650_NMC_25C_0-100_0.5-2C_a',\n",
       " 'SNL_18650_NCA_35C_0-100_0.5-1C_c',\n",
       " 'SNL_18650_NCA_15C_0-100_0.5-1C_b',\n",
       " 'SNL_18650_NCA_35C_0-100_0.5-2C_a',\n",
       " 'SNL_18650_LFP_25C_0-100_0.5-3C_d',\n",
       " 'SNL_18650_NCA_15C_0-100_0.5-1C_a',\n",
       " 'SNL_18650_NMC_35C_0-100_0.5-2C_a',\n",
       " 'SNL_18650_NMC_15C_0-100_0.5-2C_a',\n",
       " 'SNL_18650_NCA_25C_20-80_0.5-0.5C_c',\n",
       " 'SNL_18650_NMC_25C_0-100_0.5-2C_b',\n",
       " 'SNL_18650_NMC_25C_0-100_0.5-0.5C_b',\n",
       " 'SNL_18650_NMC_35C_0-100_0.5-1C_a',\n",
       " 'SNL_18650_LFP_35C_0-100_0.5-1C_b',\n",
       " 'MICH_BLForm2_pouch_NMC_45C_0-100_1-1C_b',\n",
       " 'MICH_BLForm1_pouch_NMC_45C_0-100_1-1C_a',\n",
       " 'MICH_MCForm32_pouch_NMC_45C_0-100_1-1C_b',\n",
       " 'MICH_MCForm27_pouch_NMC_25C_0-100_1-1C_g',\n",
       " 'MICH_MCForm22_pouch_NMC_25C_0-100_1-1C_b',\n",
       " 'MICH_MCForm36_pouch_NMC_45C_0-100_1-1C_f',\n",
       " 'MICH_MCForm29_pouch_NMC_25C_0-100_1-1C_i',\n",
       " 'MICH_MCForm39_pouch_NMC_45C_0-100_1-1C_i',\n",
       " 'MICH_BLForm5_pouch_NMC_45C_0-100_1-1C_e',\n",
       " 'MICH_BLForm16_pouch_NMC_25C_0-100_1-1C_f',\n",
       " 'MICH_MCForm30_pouch_NMC_25C_0-100_1-1C_j',\n",
       " 'MICH_BLForm14_pouch_NMC_25C_0-100_1-1C_d',\n",
       " 'MICH_MCForm38_pouch_NMC_45C_0-100_1-1C_h',\n",
       " 'MICH_BLForm8_pouch_NMC_45C_0-100_1-1C_h',\n",
       " 'MICH_BLForm17_pouch_NMC_25C_0-100_1-1C_g',\n",
       " 'MICH_MCForm23_pouch_NMC_25C_0-100_1-1C_c',\n",
       " 'MICH_BLForm18_pouch_NMC_25C_0-100_1-1C_h',\n",
       " 'MICH_BLForm10_pouch_NMC_25C_0-100_1-1C_j',\n",
       " 'MICH_MCForm35_pouch_NMC_45C_0-100_1-1C_e',\n",
       " 'MICH_MCForm25_pouch_NMC_25C_0-100_1-1C_e',\n",
       " 'MICH_MCForm26_pouch_NMC_25C_0-100_1-1C_f',\n",
       " 'MICH_MCForm37_pouch_NMC_45C_0-100_1-1C_g',\n",
       " 'MICH_BLForm15_pouch_NMC_25C_0-100_1-1C_e',\n",
       " 'MICH_MCForm31_pouch_NMC_45C_0-100_1-1C_a',\n",
       " 'MICH_11C_pouch_NMC_-5C_0-100_0.2-1.5C',\n",
       " 'MICH_02C_pouch_NMC_-5C_0-100_0.2-0.2C',\n",
       " 'MICH_18H_pouch_NMC_45C_50-100_0.2-1.5C',\n",
       " 'MICH_04R_pouch_NMC_25C_0-100_1.5-1.5C',\n",
       " 'MICH_03H_pouch_NMC_45C_0-100_0.2-0.2C',\n",
       " 'MICH_08C_pouch_NMC_-5C_0-100_2-2C',\n",
       " 'MICH_17C_pouch_NMC_-5C_50-100_0.2-1.5C',\n",
       " 'MICH_10R_pouch_NMC_25C_0-100_0.2-1.5C',\n",
       " 'MICH_15H_pouch_NMC_45C_50-100_0.2-0.2C',\n",
       " 'MICH_13R_pouch_NMC_25C_50-100_0.2-0.2C',\n",
       " 'MICH_14C_pouch_NMC_-5C_50-100_0.2-0.2C',\n",
       " 'MICH_12H_pouch_NMC_45C_0-100_0.2-1.5C',\n",
       " 'XJTU_2C_battery-6',\n",
       " 'XJTU_2C_battery-2',\n",
       " 'XJTU_3C_battery-2',\n",
       " 'XJTU_3C_battery-3',\n",
       " 'XJTU_3C_battery-10',\n",
       " 'XJTU_3C_battery-13',\n",
       " 'XJTU_3C_battery-7',\n",
       " 'XJTU_3C_battery-15',\n",
       " 'XJTU_3C_battery-4',\n",
       " 'XJTU_3C_battery-5',\n",
       " 'XJTU_3C_battery-11',\n",
       " 'XJTU_3C_battery-6',\n",
       " 'XJTU_2C_battery-1',\n",
       " 'XJTU_2C_battery-8',\n",
       " 'XJTU_2C_battery-4',\n",
       " 'ZN-coin_202_20231213213655_03_3',\n",
       " 'ZN-coin_202_20231213213655_03_4',\n",
       " 'ZN-coin_202_20231213213655_03_5',\n",
       " 'ZN-coin_204-1_20231205230212_07_1',\n",
       " 'ZN-coin_204-3_20231205230221_07_3',\n",
       " 'ZN-coin_205-2_20231205230234_07_5',\n",
       " 'ZN-coin_402-2_20231209225727_01_2',\n",
       " 'ZN-coin_403-1_20231209225922_01_4',\n",
       " 'ZN-coin_404-3_20231209231250_08_1',\n",
       " 'ZN-coin_405-1_20231209231331_08_2',\n",
       " 'ZN-coin_405-2_20231209231413_08_3',\n",
       " 'ZN-coin_405-3_20231209231450_08_4',\n",
       " 'ZN-coin_407-1_20231209231725_08_8',\n",
       " 'ZN-coin_407-3_20231209231841_02_2',\n",
       " 'ZN-coin_408-1_20231209231918_02_3',\n",
       " 'ZN-coin_408-2_20231209231947_02_4',\n",
       " 'ZN-coin_408-3_20231209232028_05_1',\n",
       " 'ZN-coin_409-1_20231209232338_05_2',\n",
       " 'ZN-coin_409-2_20231209232422_05_3',\n",
       " 'ZN-coin_409-3_20231209232500_05_4',\n",
       " 'ZN-coin_410-1_20231209232559_09_1',\n",
       " 'ZN-coin_410-3_20231209232707_09_3',\n",
       " 'ZN-coin_412-3_20231209233120_06_1',\n",
       " 'ZN-coin_414-2_20231209233354_06_6',\n",
       " 'ZN-coin_415-2_20231209233606_10_1',\n",
       " 'ZN-coin_416-3_20231209233856_10_5',\n",
       " 'ZN-coin_417-3_20231209234058_10_8',\n",
       " 'ZN-coin_418-1_20231209234141_11_1',\n",
       " 'ZN-coin_418-3_20231209234252_11_3',\n",
       " 'ZN-coin_420-3_20231205230017_01_3',\n",
       " 'ZN-coin_422-3_20231205230049_02_1',\n",
       " 'ZN-coin_423-1_20231205230055_02_2',\n",
       " 'ZN-coin_425-2_20231205230124_03_1',\n",
       " 'ZN-coin_428-1_20231212185048_01_2',\n",
       " 'ZN-coin_429-2_20231212185157_01_8',\n",
       " 'ZN-coin_430-1_20231212185250_02_6',\n",
       " 'ZN-coin_432-2_20231227204437_01_2',\n",
       " 'ZN-coin_433-1_20231227204534_01_4',\n",
       " 'ZN-coin_433-2_20231227204539_01_5',\n",
       " 'ZN-coin_434-1_20231227204606_01_7',\n",
       " 'ZN-coin_434-2_20231227204612_01_8',\n",
       " 'ZN-coin_434-3_20231227204618_03_1',\n",
       " 'ZN-coin_435-2_20231227204630_03_3',\n",
       " 'ZN-coin_435-3_20231227204635_03_4',\n",
       " 'ZN-coin_436-3_20231227204657_03_7',\n",
       " 'ZN-coin_437-1_20231227204706_03_8',\n",
       " 'ZN-coin_437-3_20231227204717_04_2',\n",
       " 'ZN-coin_438-1_20231227204743_04_3',\n",
       " 'ZN-coin_438-2_20231227204748_04_4',\n",
       " 'ZN-coin_439-2_20231227204810_04_7',\n",
       " 'ZN-coin_439-3_20231227204817_04_8',\n",
       " 'ZN-coin_440-2_20231227204832_08_2',\n",
       " 'ZN-coin_442-1_20240104212418_09_1',\n",
       " 'ZN-coin_442-3_20240104212433_09_3',\n",
       " 'ZN-coin_443-2_20240104212500_09_5',\n",
       " 'ZN-coin_445-1_20240104212517_09_7',\n",
       " 'ZN-coin_450-1_20240116203402_01_2_Batch-3',\n",
       " 'ZN-coin_450-2_20240116203410_01_4_Batch-3',\n",
       " 'ZN-coin_450-3_20240116203417_03_3_Batch-3',\n",
       " 'ZN-coin_451-1_20240116203425_03_4_Batch-3',\n",
       " 'CALB_35_B229',\n",
       " 'CALB_35_B173',\n",
       " 'CALB_35_B228',\n",
       " 'CALB_0_B184',\n",
       " 'CALB_35_B248',\n",
       " 'CALB_35_B227',\n",
       " 'CALB_0_B185',\n",
       " 'CALB_35_B249',\n",
       " 'CALB_35_B223',\n",
       " 'CALB_35_B224',\n",
       " 'CALB_0_B189',\n",
       " 'CALB_35_B250',\n",
       " 'CALB_0_B188',\n",
       " 'CALB_45_B256',\n",
       " 'CALB_0_B183',\n",
       " 'CALB_35_B175',\n",
       " 'CALB_0_B190',\n",
       " 'NA-ion_270040-1-2-63',\n",
       " 'NA-ion_270040-1-5-60',\n",
       " 'NA-ion_270040-1-7-58',\n",
       " 'NA-ion_270040-1-8-57',\n",
       " 'NA-ion_270040-2-2-12',\n",
       " 'NA-ion_270040-2-5-12',\n",
       " 'NA-ion_270040-3-1-56',\n",
       " 'NA-ion_270040-3-5-52',\n",
       " 'NA-ion_270040-5-2-38',\n",
       " 'NA-ion_270040-3-8-49',\n",
       " 'NA-ion_270040-5-1-39',\n",
       " 'NA-ion_270040-5-3-37',\n",
       " 'NA-ion_270040-5-6-34',\n",
       " 'NA-ion_270040-5-7-33',\n",
       " 'NA-ion_270040-6-2-30',\n",
       " 'NA-ion_270040-6-6-26',\n",
       " 'NA-ion_270040-7-1-23',\n",
       " 'NA-ion_270040-8-5-16',\n",
       " 'NA-ion_270040-3-3-54',\n",
       " 'NA-ion_270040-6-8-24']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataset = 'MIX_all' # [MIX_large, MIX_all, MIX_all2024, MIX_all42]\n",
    "if target_dataset == 'MIX_large':\n",
    "    cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_large_train_files]\n",
    "elif target_dataset == 'MIX_all':\n",
    "    cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_train_files]\n",
    "elif target_dataset == 'MIX_all2024':\n",
    "    cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_2024_train_files]\n",
    "elif target_dataset == 'MIX_all42':\n",
    "    cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_42_train_files]\n",
    "cell_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(B, seq_len):\n",
    "    '''\n",
    "    return:\n",
    "        casual mask: [B, L, L]. 0 indicates masked.\n",
    "    '''\n",
    "    # Create a lower triangular matrix of shape (seq_len, seq_len)\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))  # (L, L)\n",
    "    mask = mask.unsqueeze(0).expand(B, -1, -1)\n",
    "    return mask\n",
    "\n",
    "def last_token_pool(last_hidden_states, attention_mask):\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery:{query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770adb44340047b1a6ce95ed66401993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loader the tokenizer and model\n",
    "\n",
    "# '/data/LLMs/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'\n",
    "# '/data/LLMs/models--Qwen--Qwen3-Embedding-8B/snapshots/a3d38e32b9c835d5b3d0d0a3ef3c133bbea92539'\n",
    "# '/data/LLMs/models--Qwen--Qwen3-Embedding-0.6B/snapshots/744169034862c8eec56628663995004342e4e449'\n",
    "# 'Qwen/Qwen3-Embedding-0.6B'\n",
    "LLM_path = '/data/LLMs/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'\n",
    "llama_config = AutoConfig.from_pretrained(LLM_path)\n",
    "# language_model = AutoModel.from_pretrained(\n",
    "#             LLM_path,\n",
    "#             # 'huggyllama/llama-7b',\n",
    "#             trust_remote_code=True,\n",
    "#             local_files_only=True,\n",
    "#             config=llama_config,\n",
    "#             load_in_4bit=True                                                                                                                                                                  \n",
    "#         )\n",
    "if 'Qwen3-Embedding-0.6B' in LLM_path:\n",
    "    language_model = AutoModel.from_pretrained(\n",
    "                LLM_path\n",
    "            ).cuda()\n",
    "else:\n",
    "    language_model = AutoModel.from_pretrained(\n",
    "                LLM_path,\n",
    "                # 'huggyllama/llama-7b',\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True,\n",
    "                config=llama_config,\n",
    "                load_in_4bit=True\n",
    "            )\n",
    "if 'Llama' in LLM_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    LLM_path,\n",
    "                    # 'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True, \n",
    "                    pad_token='<|endoftext|>'\n",
    "                )\n",
    "    tokenizer.padding_side = 'right' # set the padding side\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_path, padding_side='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trf/envs/py310/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICH_18H_pouch_NMC_45C_50-100_0.2-1.5C.pkl\n",
      "MICH_17C_pouch_NMC_-5C_50-100_0.2-1.5C.pkl\n",
      "MICH_15H_pouch_NMC_45C_50-100_0.2-0.2C.pkl\n",
      "MICH_13R_pouch_NMC_25C_50-100_0.2-0.2C.pkl\n",
      "MICH_14C_pouch_NMC_-5C_50-100_0.2-0.2C.pkl\n",
      "ZN-coin_403-1_20231209225922_01_4.pkl\n",
      "ZN-coin_410-1_20231209232559_09_1.pkl\n",
      "ZN-coin_418-1_20231209234141_11_1.pkl\n",
      "ZN-coin_402-1_20231209225636_01_1.pkl\n",
      "MICH_16R_pouch_NMC_25C_50-100_0.2-1.5C.pkl\n",
      "ZN-coin_428-2_20231212185058_01_4.pkl\n"
     ]
    }
   ],
   "source": [
    "def read_cell_data_according_to_prefix(file_name, root_path):\n",
    "    prefix = file_name.split('_')[0]\n",
    "    if prefix == 'MICH':\n",
    "        with open(f'{root_path}/Life labels/total_MICH_labels.json') as f:\n",
    "            life_labels = json.load(f)\n",
    "    elif prefix.startswith('Tongji'):\n",
    "        file_name = file_name.replace('--', '-#')\n",
    "        with open(f'{root_path}/Life labels/Tongji_labels.json') as f:\n",
    "            life_labels = json.load(f)\n",
    "    else:\n",
    "        with open(f'{root_path}/Life labels/{prefix}_labels.json') as f:\n",
    "            life_labels = json.load(f)\n",
    "    if file_name in life_labels:\n",
    "        eol = life_labels[file_name]\n",
    "    else:\n",
    "        eol = None\n",
    "\n",
    "    return eol\n",
    "\n",
    "def get_features_from_cellNames(cell_names):\n",
    "    cellName_prompt = {}\n",
    "    total_labels = []\n",
    "    for cell_name in cell_names:\n",
    "        # bg_prompt = (\n",
    "        #             f\"Task description: You are an expert in predicting battery cycle life. \" \n",
    "        #             f\"The cycle life is the number of cycles until the battery's discharge capacity reaches 80% of its nominal capacity. \"\n",
    "        #             f\"The discharge capacity is calculated under the described operating condition. \"\n",
    "        #             f\"Please directly output the cycle life of the battery based on the provided data. \"\n",
    "        #             )\n",
    "        if 'CALB' in cell_name:\n",
    "            bg_prompt = (\n",
    "                        f\"Task description: \" \n",
    "                        f\"The target is the number of cycles until the battery's discharge capacity reaches 90% of its nominal capacity. \"\n",
    "                        f\"The discharge capacity is calculated under the described operating condition. \"\n",
    "                        f\"Please directly output the target of the battery based on the provided data. \"\n",
    "                        )\n",
    "        else:\n",
    "            bg_prompt = (\n",
    "                        f\"Task description: \" \n",
    "                        f\"The target is the number of cycles until the battery's discharge capacity reaches 80% of its nominal capacity. \"\n",
    "                        f\"The discharge capacity is calculated under the described operating condition. \"\n",
    "                        f\"Please directly output the target of the battery based on the provided data. \"\n",
    "                        )\n",
    "        helper = Mapping_helper(prompt_type='PROTOCOL', cell_name=cell_name)\n",
    "        tmp_prompt = bg_prompt + helper.do_mapping()\n",
    "        eol = read_cell_data_according_to_prefix(cell_name+'.pkl', '/data/trf/python_works/BatteryLife/dataset')\n",
    "        if eol is None:\n",
    "            print(cell_name+'.pkl')\n",
    "            continue\n",
    "        total_labels.append(eol)\n",
    "        # Llama-instruct\n",
    "        if 'Llama' in LLM_path:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": tmp_prompt}\n",
    "            ]\n",
    "\n",
    "            tmp_prompt = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            res = tokenizer(tmp_prompt, return_tensors=\"pt\")\n",
    "            input_ids, attention_mask = res['input_ids'][:,1:], res['attention_mask'][:,1:]\n",
    "            llama_enc_out = language_model.get_input_embeddings()(input_ids) # [1, L', d_llm]\n",
    "            \n",
    "            cache_position = torch.arange(\n",
    "                    0, 0 + llama_enc_out.shape[1], device=llama_enc_out.device\n",
    "                )\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "            DLP_attention_mask = attention_mask.unsqueeze(1) # [B, 1, L]\n",
    "            DLP_attention_mask = DLP_attention_mask.expand(-1, DLP_attention_mask.shape[-1], -1) # [B, L, L]\n",
    "            DLP_attention_mask = DLP_attention_mask.unsqueeze(1) # [B, 1, L, L]\n",
    "            \n",
    "            casual_mask = create_causal_mask(1, llama_enc_out.shape[1])\n",
    "            casual_mask = casual_mask.unsqueeze(1) # [B, 1, L, L]\n",
    "\n",
    "            DLP_attention_mask = torch.where(casual_mask.to(DLP_attention_mask.device)==1, DLP_attention_mask, torch.zeros_like(DLP_attention_mask))\n",
    "            DLP_attention_mask = DLP_attention_mask==1 # set True to allow attention to attend to\n",
    "\n",
    "            hidden_states = language_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "            # hidden_states = llama_enc_out\n",
    "            # for i, layer in enumerate(language_model.layers):\n",
    "            #     res = layer(hidden_states=hidden_states, position_ids=position_ids, attention_mask=DLP_attention_mask, cache_position=cache_position)\n",
    "            #     hidden_states = res[0]\n",
    "\n",
    "            features = hidden_states[:,-1,:].detach().cpu().numpy().reshape(1, -1)\n",
    "        elif 'Qwen3' in LLM_path:\n",
    "            tmp_prompt = [get_detailed_instruct('classification', tmp_prompt)]\n",
    "            res = tokenizer(\n",
    "                tmp_prompt,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=8192,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            res.to(language_model.device)\n",
    "            outputs = language_model(**res)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, res['attention_mask'])\n",
    "            features = embeddings.detach().cpu().numpy().reshape(1, -1)\n",
    "        else:\n",
    "            raise Exception(f'{LLM_path} is not supported here')\n",
    "\n",
    "        \n",
    "    \n",
    "        cellName_prompt[cell_name] = features\n",
    "    return cellName_prompt, total_labels\n",
    "\n",
    "cellName_prompt, total_labels = get_features_from_cellNames(cell_names)\n",
    "\n",
    "# get the features from validation and testing sets\n",
    "if target_dataset == 'MIX_large':\n",
    "    val_cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_large_val_files]\n",
    "    test_cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_large_test_files]\n",
    "elif target_dataset == 'MIX_all':\n",
    "    val_cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_val_files]\n",
    "    test_cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_test_files]\n",
    "elif target_dataset == 'MIX_all2024':\n",
    "    val_cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_2024_val_files]\n",
    "    test_cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_2024_test_files]\n",
    "elif target_dataset == 'MIX_all42':\n",
    "    val_cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_42_val_files]\n",
    "    test_cell_names = [i.split('.pkl')[0] for i in split_recorder.MIX_all_42_test_files]\n",
    "val_cellName_prompt, val_total_labels = get_features_from_cellNames(val_cell_names)\n",
    "test_cellName_prompt, test_total_labels = get_features_from_cellNames(test_cell_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## Export the domain-knowledge prompt embeddings of the samples\n",
    "save_path = '/data/trf/python_works/BatteryLife/dataset/'\n",
    "\n",
    "\n",
    "if 'Llama' in LLM_path:\n",
    "    name_comment = 'Llama'\n",
    "    with open(f'{save_path}training_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(cellName_prompt, f)\n",
    "    with open(f'{save_path}validation_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(val_cellName_prompt, f)\n",
    "    with open(f'{save_path}testing_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(test_cellName_prompt, f)\n",
    "elif 'Qwen3-Embedding-4B' in LLM_path:\n",
    "    name_comment = 'Qwen3_4B'\n",
    "    with open(f'{save_path}training_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(cellName_prompt, f)\n",
    "    with open(f'{save_path}validation_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(val_cellName_prompt, f)\n",
    "    with open(f'{save_path}testing_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(test_cellName_prompt, f)\n",
    "elif 'Qwen3-Embedding-8B' in LLM_path:\n",
    "    name_comment = 'Qwen3_8B'\n",
    "    with open(f'{save_path}training_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(cellName_prompt, f)\n",
    "    with open(f'{save_path}validation_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(val_cellName_prompt, f)\n",
    "    with open(f'{save_path}testing_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(test_cellName_prompt, f)\n",
    "elif 'Qwen3-Embedding-0.6B' in LLM_path:\n",
    "    name_comment = 'Qwen3_0.6B'\n",
    "    with open(f'{save_path}training_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(cellName_prompt, f)\n",
    "    with open(f'{save_path}validation_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(val_cellName_prompt, f)\n",
    "    with open(f'{save_path}testing_DKP_embed_all_{name_comment}.pkl', 'wb') as f:\n",
    "        pickle.dump(test_cellName_prompt, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmpy311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
