{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "This notebook is used to obtain the embeddings of the domain-knowledge prompt for one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import transformers\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "import json\n",
    "import torch\n",
    "from data_provider.data_split_recorder import split_recorder\n",
    "from Prompts.Mapping_helper import Mapping_helper\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, AutoTokenizer, AutoModel, AutoConfig, Phi3Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ax_linewidth(ax, bw=1.5):\n",
    "    ax.spines['bottom'].set_linewidth(bw)\n",
    "    ax.spines['left'].set_linewidth(bw)\n",
    "    ax.spines['top'].set_linewidth(bw)\n",
    "    ax.spines['right'].set_linewidth(bw)\n",
    "\n",
    "def set_ax_font_size(ax, fontsize=10):\n",
    "    ax.tick_params(axis='y',\n",
    "                 labelsize=fontsize # y轴字体大小设置\n",
    "                  ) \n",
    "    ax.tick_params(axis='x',\n",
    "                 labelsize=fontsize # x轴字体大小设置\n",
    "                  ) \n",
    "\n",
    "def set_draft(the_plt, other_ax=''):\n",
    "    ax = the_plt.gca()\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    if other_ax:\n",
    "        other_ax.axes.xaxis.set_ticklabels([])\n",
    "        other_ax.axes.yaxis.set_ticklabels([])\n",
    "        other_ax.set_ylabel('')\n",
    "        other_ax.set_xlabel('')\n",
    "\n",
    "def set_draft_fig(fig):\n",
    "    for ax in fig.axes:\n",
    "        ax.axes.xaxis.set_ticklabels([])\n",
    "        ax.axes.yaxis.set_ticklabels([])\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stanford_Nova_Regular_318.pkl',\n",
       " 'Stanford_Nova_Regular_319.pkl',\n",
       " 'Stanford_Nova_Regular_320.pkl',\n",
       " 'Stanford_Nova_Regular_187.pkl',\n",
       " 'Stanford_Nova_Regular_188.pkl',\n",
       " 'Stanford_Nova_Regular_189.pkl',\n",
       " 'Stanford_Nova_Regular_136.pkl',\n",
       " 'Stanford_Nova_Regular_137.pkl',\n",
       " 'Stanford_Nova_Regular_138.pkl',\n",
       " 'Stanford_Nova_Regular_207.pkl',\n",
       " 'Stanford_Nova_Regular_288.pkl',\n",
       " 'Stanford_Nova_Regular_289.pkl',\n",
       " 'Stanford_Nova_Regular_290.pkl',\n",
       " 'Stanford_Nova_Regular_269.pkl',\n",
       " 'Stanford_Nova_Regular_297.pkl',\n",
       " 'Stanford_Nova_Regular_299.pkl',\n",
       " 'Stanford_Nova_Regular_148.pkl',\n",
       " 'Stanford_Nova_Regular_149.pkl',\n",
       " 'Stanford_Nova_Regular_150.pkl',\n",
       " 'Stanford_Nova_Regular_213.pkl',\n",
       " 'Stanford_Nova_Regular_127.pkl',\n",
       " 'Stanford_Nova_Regular_128.pkl',\n",
       " 'Stanford_Nova_Regular_129.pkl',\n",
       " 'Stanford_Nova_Regular_312.pkl',\n",
       " 'Stanford_Nova_Regular_313.pkl',\n",
       " 'Stanford_Nova_Regular_314.pkl',\n",
       " 'Stanford_Nova_Regular_223.pkl',\n",
       " 'Stanford_Nova_Regular_160.pkl',\n",
       " 'Stanford_Nova_Regular_161.pkl',\n",
       " 'Stanford_Nova_Regular_162.pkl',\n",
       " 'Stanford_Nova_Regular_112.pkl',\n",
       " 'Stanford_Nova_Regular_113.pkl',\n",
       " 'Stanford_Nova_Regular_114.pkl',\n",
       " 'Stanford_Nova_Regular_178.pkl',\n",
       " 'Stanford_Nova_Regular_179.pkl',\n",
       " 'Stanford_Nova_Regular_180.pkl',\n",
       " 'Stanford_Nova_Regular_142.pkl',\n",
       " 'Stanford_Nova_Regular_143.pkl',\n",
       " 'Stanford_Nova_Regular_144.pkl',\n",
       " 'Stanford_Nova_Regular_115.pkl',\n",
       " 'Stanford_Nova_Regular_116.pkl',\n",
       " 'Stanford_Nova_Regular_117.pkl',\n",
       " 'Stanford_Nova_Regular_172.pkl',\n",
       " 'Stanford_Nova_Regular_173.pkl',\n",
       " 'Stanford_Nova_Regular_174.pkl',\n",
       " 'Stanford_Nova_Regular_202.pkl',\n",
       " 'Stanford_Nova_Regular_204.pkl',\n",
       " 'Stanford_Nova_Regular_130.pkl',\n",
       " 'Stanford_Nova_Regular_131.pkl',\n",
       " 'Stanford_Nova_Regular_324.pkl',\n",
       " 'Stanford_Nova_Regular_325.pkl',\n",
       " 'Stanford_Nova_Regular_326.pkl',\n",
       " 'Stanford_Nova_Regular_282.pkl',\n",
       " 'Stanford_Nova_Regular_283.pkl',\n",
       " 'Stanford_Nova_Regular_284.pkl',\n",
       " 'Stanford_Nova_Regular_145.pkl',\n",
       " 'Stanford_Nova_Regular_146.pkl',\n",
       " 'Stanford_Nova_Regular_147.pkl',\n",
       " 'Stanford_Nova_Regular_134.pkl',\n",
       " 'Stanford_Nova_Regular_135.pkl',\n",
       " 'Stanford_Nova_Regular_279.pkl',\n",
       " 'Stanford_Nova_Regular_280.pkl',\n",
       " 'Stanford_Nova_Regular_281.pkl',\n",
       " 'Stanford_Nova_Regular_154.pkl',\n",
       " 'Stanford_Nova_Regular_155.pkl',\n",
       " 'Stanford_Nova_Regular_156.pkl',\n",
       " 'Stanford_Nova_Regular_118.pkl',\n",
       " 'Stanford_Nova_Regular_119.pkl',\n",
       " 'Stanford_Nova_Regular_120.pkl',\n",
       " 'Stanford_Nova_Regular_306.pkl',\n",
       " 'Stanford_Nova_Regular_307.pkl',\n",
       " 'Stanford_Nova_Regular_308.pkl',\n",
       " 'Stanford_Nova_Regular_163.pkl',\n",
       " 'Stanford_Nova_Regular_165.pkl',\n",
       " 'Stanford_Nova_Regular_303.pkl',\n",
       " 'Stanford_Nova_Regular_304.pkl',\n",
       " 'Stanford_Nova_Regular_305.pkl',\n",
       " 'Stanford_Nova_Regular_309.pkl',\n",
       " 'Stanford_Nova_Regular_310.pkl',\n",
       " 'Stanford_Nova_Regular_311.pkl',\n",
       " 'Stanford_Nova_Regular_157.pkl',\n",
       " 'Stanford_Nova_Regular_158.pkl',\n",
       " 'Stanford_Nova_Regular_159.pkl',\n",
       " 'Stanford_Nova_Regular_151.pkl',\n",
       " 'Stanford_Nova_Regular_152.pkl',\n",
       " 'Stanford_Nova_Regular_228.pkl',\n",
       " 'Stanford_Nova_Regular_203.pkl',\n",
       " 'Stanford_Nova_Regular_215.pkl',\n",
       " 'Stanford_Nova_Regular_225.pkl',\n",
       " 'Stanford_Nova_Regular_222.pkl',\n",
       " 'Stanford_Nova_Regular_226.pkl',\n",
       " 'Stanford_Nova_Regular_221.pkl',\n",
       " 'Stanford_Nova_Regular_211.pkl',\n",
       " 'Stanford_Nova_Regular_219.pkl',\n",
       " 'Stanford_Nova_Regular_229.pkl',\n",
       " 'Stanford_Nova_Regular_193.pkl',\n",
       " 'Stanford_Nova_Regular_200.pkl',\n",
       " 'Stanford_Nova_Regular_205.pkl',\n",
       " 'Stanford_Nova_Regular_230.pkl',\n",
       " 'Stanford_Nova_Regular_196.pkl',\n",
       " 'Stanford_Nova_Regular_216.pkl',\n",
       " 'Stanford_Nova_Regular_220.pkl',\n",
       " 'Stanford_Nova_Regular_201.pkl',\n",
       " 'Stanford_Nova_Regular_101.pkl',\n",
       " 'Stanford_Nova_Regular_192.pkl',\n",
       " 'Stanford_Nova_Regular_208.pkl',\n",
       " 'Stanford_Nova_Regular_102.pkl',\n",
       " 'Stanford_Nova_Regular_210.pkl',\n",
       " 'Stanford_Nova_Regular_212.pkl',\n",
       " 'Stanford_Nova_Regular_199.pkl',\n",
       " 'Stanford_Nova_Regular_217.pkl',\n",
       " 'Stanford_Nova_Regular_194.pkl',\n",
       " 'Stanford_Nova_Regular_214.pkl',\n",
       " 'Stanford_Nova_Regular_195.pkl',\n",
       " 'Stanford_Nova_Regular_191.pkl',\n",
       " 'Stanford_Nova_Regular_224.pkl',\n",
       " 'Stanford_Nova_Regular_206.pkl',\n",
       " 'Stanford_Nova_Regular_227.pkl',\n",
       " 'Stanford_Nova_Regular_103.pkl',\n",
       " 'Stanford_Nova_Regular_104.pkl',\n",
       " 'Stanford_Nova_Regular_105.pkl',\n",
       " 'Stanford_Nova_Regular_285.pkl',\n",
       " 'Stanford_Nova_Regular_286.pkl',\n",
       " 'Stanford_Nova_Regular_287.pkl',\n",
       " 'Stanford_Nova_Regular_276.pkl',\n",
       " 'Stanford_Nova_Regular_277.pkl',\n",
       " 'Stanford_Nova_Regular_278.pkl',\n",
       " 'Stanford_Nova_Regular_300.pkl',\n",
       " 'Stanford_Nova_Regular_301.pkl',\n",
       " 'Stanford_Nova_Regular_302.pkl',\n",
       " 'Stanford_Nova_Regular_124.pkl',\n",
       " 'Stanford_Nova_Regular_125.pkl',\n",
       " 'Stanford_Nova_Regular_126.pkl',\n",
       " 'Stanford_Nova_Regular_198.pkl',\n",
       " 'Stanford_Nova_Regular_184.pkl',\n",
       " 'Stanford_Nova_Regular_185.pkl',\n",
       " 'Stanford_Nova_Regular_186.pkl',\n",
       " 'Stanford_Nova_Regular_321.pkl',\n",
       " 'Stanford_Nova_Regular_322.pkl',\n",
       " 'Stanford_Nova_Regular_323.pkl',\n",
       " 'Stanford_Nova_Regular_109.pkl',\n",
       " 'Stanford_Nova_Regular_110.pkl',\n",
       " 'Stanford_Nova_Regular_209.pkl',\n",
       " 'Stanford_Nova_Regular_190.pkl',\n",
       " 'Stanford_Nova_Regular_181.pkl',\n",
       " 'Stanford_Nova_Regular_182.pkl',\n",
       " 'Stanford_Nova_Regular_183.pkl',\n",
       " 'Stanford_Nova_Regular_294.pkl',\n",
       " 'Stanford_Nova_Regular_295.pkl',\n",
       " 'Stanford_Nova_Regular_296.pkl',\n",
       " 'Stanford_Nova_Regular_166.pkl',\n",
       " 'Stanford_Nova_Regular_167.pkl',\n",
       " 'Stanford_Nova_Regular_168.pkl',\n",
       " 'Stanford_Nova_Regular_139.pkl',\n",
       " 'Stanford_Nova_Regular_140.pkl',\n",
       " 'Stanford_Nova_Regular_141.pkl',\n",
       " 'Stanford_Nova_Regular_121.pkl',\n",
       " 'Stanford_Nova_Regular_122.pkl',\n",
       " 'Stanford_Nova_Regular_123.pkl',\n",
       " 'Stanford_Nova_Regular_106.pkl',\n",
       " 'Stanford_Nova_Regular_107.pkl',\n",
       " 'Stanford_Nova_Regular_108.pkl',\n",
       " 'Stanford_Nova_Regular_315.pkl',\n",
       " 'Stanford_Nova_Regular_316.pkl',\n",
       " 'Stanford_Nova_Regular_317.pkl',\n",
       " 'Stanford_Nova_Regular_169.pkl',\n",
       " 'Stanford_Nova_Regular_170.pkl',\n",
       " 'Stanford_Nova_Regular_171.pkl',\n",
       " 'Stanford_Nova_Regular_175.pkl',\n",
       " 'Stanford_Nova_Regular_176.pkl',\n",
       " 'Stanford_Nova_Regular_177.pkl',\n",
       " 'Stanford_Nova_Regular_270.pkl',\n",
       " 'Stanford_Nova_Regular_271.pkl',\n",
       " 'Stanford_Nova_Regular_272.pkl',\n",
       " 'Stanford_Nova_Regular_291.pkl',\n",
       " 'Stanford_Nova_Regular_292.pkl',\n",
       " 'Stanford_Nova_Regular_293.pkl',\n",
       " 'Stanford_Nova_Regular_273.pkl',\n",
       " 'Stanford_Nova_Regular_274.pkl',\n",
       " 'Stanford_Nova_Regular_275.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dataset = 'Stanford_formation' \n",
    "if target_dataset == 'Stanford_formation':\n",
    "    cell_names = split_recorder.Stanford_formation_45_train_files_2021 + split_recorder.Stanford_formation_45_val_files_2021 + split_recorder.Stanford_formation_45_test_files_2021 # Please include all file names here. If you have a new file, you can add it.\n",
    "cell_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(B, seq_len):\n",
    "    '''\n",
    "    return:\n",
    "        casual mask: [B, L, L]. 0 indicates masked.\n",
    "    '''\n",
    "    # Create a lower triangular matrix of shape (seq_len, seq_len)\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))  # (L, L)\n",
    "    mask = mask.unsqueeze(0).expand(B, -1, -1)\n",
    "    return mask\n",
    "\n",
    "def last_token_pool(last_hidden_states, attention_mask):\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery:{query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6fb812b9424e04b746e7c7f63449a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loader the tokenizer and model\n",
    "\n",
    "# '/data/LLMs/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'\n",
    "# '/data/LLMs/models--Qwen--Qwen3-Embedding-8B/snapshots/a3d38e32b9c835d5b3d0d0a3ef3c133bbea92539'\n",
    "# '/data/LLMs/models--Qwen--Qwen3-Embedding-0.6B/snapshots/744169034862c8eec56628663995004342e4e449'\n",
    "# 'Qwen/Qwen3-Embedding-0.6B'\n",
    "LLM_path = '/data/LLMs/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'\n",
    "llama_config = AutoConfig.from_pretrained(LLM_path)\n",
    "if 'Qwen3-Embedding-0.6B' in LLM_path:\n",
    "    language_model = AutoModel.from_pretrained(\n",
    "                LLM_path\n",
    "            ).cuda()\n",
    "else:\n",
    "    language_model = AutoModel.from_pretrained(\n",
    "                LLM_path,\n",
    "                # 'huggyllama/llama-7b',\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True,\n",
    "                config=llama_config,\n",
    "                load_in_4bit=True\n",
    "            )\n",
    "if 'Llama' in LLM_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    LLM_path,\n",
    "                    # 'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True, \n",
    "                    pad_token='<|endoftext|>'\n",
    "                )\n",
    "    tokenizer.padding_side = 'right' # set the padding side\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_path, padding_side='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home/trf/envs/llmpy311/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def read_cell_data_according_to_prefix(file_name, root_path):\n",
    "    prefix = file_name.split('_')[0]\n",
    "    if prefix == 'MICH':\n",
    "        with open(f'{root_path}/Life labels/total_MICH_labels.json') as f:\n",
    "            life_labels = json.load(f)\n",
    "    elif prefix.startswith('Tongji'):\n",
    "        file_name = file_name.replace('--', '-#')\n",
    "        with open(f'{root_path}/Life labels/Tongji_labels.json') as f:\n",
    "            life_labels = json.load(f)\n",
    "    elif prefix.startswith('Stanford'):\n",
    "        if target_dataset == 'Stanford_formation':\n",
    "            with open(f'{root_path}/Life labels/Stanford_2_labels.json') as f:\n",
    "                life_labels = json.load(f)\n",
    "        else:\n",
    "            with open(f'{root_path}/Life labels/{prefix}_labels.json') as f:\n",
    "                life_labels = json.load(f)\n",
    "    else:\n",
    "        with open(f'{root_path}/Life labels/{prefix}_labels.json') as f:\n",
    "            life_labels = json.load(f)\n",
    "    if file_name in life_labels:\n",
    "        eol = life_labels[file_name]\n",
    "    else:\n",
    "        eol = None\n",
    "\n",
    "    return eol\n",
    "\n",
    "def get_features_from_cellNames(cell_names):\n",
    "    cellName_prompt = {}\n",
    "    total_labels = []\n",
    "    for cell_name in cell_names:\n",
    "        # bg_prompt = (\n",
    "        #             f\"Task description: You are an expert in predicting battery cycle life. \" \n",
    "        #             f\"The cycle life is the number of cycles until the battery's discharge capacity reaches 80% of its nominal capacity. \"\n",
    "        #             f\"The discharge capacity is calculated under the described operating condition. \"\n",
    "        #             f\"Please directly output the cycle life of the battery based on the provided data. \"\n",
    "        #             )\n",
    "        if 'CALB' in cell_name:\n",
    "            bg_prompt = (\n",
    "                        f\"Task description: \" \n",
    "                        f\"The target is the number of cycles until the battery's discharge capacity reaches 90% of its nominal capacity. \"\n",
    "                        f\"The discharge capacity is calculated under the described operating condition. \"\n",
    "                        f\"Please directly output the target of the battery based on the provided data. \"\n",
    "                        )\n",
    "        else:\n",
    "            bg_prompt = (\n",
    "                        f\"Task description: \" \n",
    "                        f\"The target is the number of cycles until the battery's discharge capacity reaches 80% of its nominal capacity. \"\n",
    "                        f\"The discharge capacity is calculated under the described operating condition. \"\n",
    "                        f\"Please directly output the target of the battery based on the provided data. \"\n",
    "                        )\n",
    "        \n",
    "        cell_name = cell_name.split('.pkl')[0]\n",
    "        helper = Mapping_helper(prompt_type='PROTOCOL', cell_name=cell_name)\n",
    "        tmp_prompt = bg_prompt + helper.do_mapping()\n",
    "        eol = read_cell_data_according_to_prefix(cell_name+'.pkl', '/data/trf/python_works/BatteryLife/dataset')\n",
    "        if eol is None:\n",
    "            print(cell_name+'.pkl')\n",
    "            continue\n",
    "        total_labels.append(eol)\n",
    "        # Llama-instruct\n",
    "        if 'Llama' in LLM_path:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": tmp_prompt}\n",
    "            ]\n",
    "\n",
    "            tmp_prompt = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            res = tokenizer(tmp_prompt, return_tensors=\"pt\")\n",
    "            input_ids, attention_mask = res['input_ids'][:,1:], res['attention_mask'][:,1:]\n",
    "            llama_enc_out = language_model.get_input_embeddings()(input_ids) # [1, L', d_llm]\n",
    "            \n",
    "            cache_position = torch.arange(\n",
    "                    0, 0 + llama_enc_out.shape[1], device=llama_enc_out.device\n",
    "                )\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "            DLP_attention_mask = attention_mask.unsqueeze(1) # [B, 1, L]\n",
    "            DLP_attention_mask = DLP_attention_mask.expand(-1, DLP_attention_mask.shape[-1], -1) # [B, L, L]\n",
    "            DLP_attention_mask = DLP_attention_mask.unsqueeze(1) # [B, 1, L, L]\n",
    "            \n",
    "            casual_mask = create_causal_mask(1, llama_enc_out.shape[1])\n",
    "            casual_mask = casual_mask.unsqueeze(1) # [B, 1, L, L]\n",
    "\n",
    "            DLP_attention_mask = torch.where(casual_mask.to(DLP_attention_mask.device)==1, DLP_attention_mask, torch.zeros_like(DLP_attention_mask))\n",
    "            DLP_attention_mask = DLP_attention_mask==1 # set True to allow attention to attend to\n",
    "\n",
    "            hidden_states = language_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "            # hidden_states = llama_enc_out\n",
    "            # for i, layer in enumerate(language_model.layers):\n",
    "            #     res = layer(hidden_states=hidden_states, position_ids=position_ids, attention_mask=DLP_attention_mask, cache_position=cache_position)\n",
    "            #     hidden_states = res[0]\n",
    "\n",
    "            features = hidden_states[:,-1,:].detach().cpu().numpy().reshape(1, -1)\n",
    "        elif 'Qwen3' in LLM_path:\n",
    "            tmp_prompt = [get_detailed_instruct('classification', tmp_prompt)]\n",
    "            res = tokenizer(\n",
    "                tmp_prompt,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=8192,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            res.to(language_model.device)\n",
    "            outputs = language_model(**res)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, res['attention_mask'])\n",
    "            features = embeddings.detach().cpu().numpy().reshape(1, -1)\n",
    "        else:\n",
    "            raise Exception(f'{LLM_path} is not supported here')\n",
    "\n",
    "        \n",
    "    \n",
    "        cellName_prompt[cell_name] = features\n",
    "    return cellName_prompt, total_labels\n",
    "\n",
    "print(len(cell_names))\n",
    "cellName_prompt, total_labels = get_features_from_cellNames(cell_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "## Export the domain-knowledge prompt embeddings of the samples\n",
    "save_path = '/data/trf/python_works/BatteryLife/dataset/'\n",
    "name_comment = 'Llama'\n",
    "print(len(cellName_prompt))\n",
    "with open(f'{save_path}training_DKP_embed_all_{name_comment}_{target_dataset}.pkl', 'wb') as f:\n",
    "    pickle.dump(cellName_prompt, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
